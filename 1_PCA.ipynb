{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c170a249",
   "metadata": {},
   "source": [
    "## I. 核心思想\n",
    "\n",
    "1. 核心思想：将一组可能线性相关的向量，通过正交变换，转换城维度更小的线性无关的向量。\n",
    "2. 本质：特征空间重构\n",
    "3. 解法：**最大投影方差**和**最小重构距离** [不同但数学上等价] \\\n",
    "思路：找原向量具有最大投影方差的方向作为新的特征空间的基。此时，原向量到它们各自在新基上的投影的距离最小。这也意味着，假如将“距离”视为“重构代价”的话，用这些投影，也就是降维后的数据重构原数据的“重构代价”最小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eba33c",
   "metadata": {},
   "source": [
    "## II. 求解需要的基础工具"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffc7405",
   "metadata": {},
   "source": [
    "1. **p维对称矩阵的性质**： \\\n",
    "a. 有p个实特征值，可能有重复值，每个特征值对应的特征空间维数=该特征值的重数 \\\n",
    "b. 有p个特征向量，且特征向量相互正交 \\\n",
    "c. 可以正交对角化，相似矩阵对角线上的元素就是对称矩阵的特征值；正交矩阵由p个特征向量构成。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b4c5a4",
   "metadata": {},
   "source": [
    "2. 样本方差矩阵S\n",
    "$$S = \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}(x_i-\\bar x)(x_i-\\bar x)^{T}，x_i是p维向量$$\n",
    "<font color=blue>**S是p维对称矩阵, 因此可以正交对角化：**</font>\n",
    "$$\n",
    "\\begin{align} \n",
    "S_{p*p} & = G^T\\Lambda G \\\\\n",
    "&  =  \\begin{pmatrix}\n",
    " | & | &  & |\\\\\n",
    " g_1 & g_2 & ... & g_p\\\\\n",
    " | & | &  &|\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    " \\lambda_1 & 0 & ... & 0  \\\\\n",
    " 0 & \\lambda_2 &  & 0  \\\\\n",
    "  &  & ... &   \\\\\n",
    " 0 & 0 & ... & \\lambda_p  \n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    " - & g_1^T & -\\\\\n",
    " - & g_2^T & - \\\\\n",
    "  & ... & \\\\\n",
    " - & g_p^T & - \n",
    "\\end{pmatrix}  \\\\\n",
    "& = \\lambda _1g_1g_1^T + \\lambda _2g_2g_2^T + ... + \\lambda _pg_pg_p^T  \\\\\n",
    "& =  {\\textstyle \\sum_{i=1}^{p}} \\lambda _ig_ig_i^T  \\\\\n",
    "& 其中，G^TG =I_p, \\lambda_1>\\lambda_2>...>\\lambda_p\n",
    "\\end{align} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be851ec4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T07:31:26.646735Z",
     "start_time": "2023-11-23T07:31:26.634076Z"
    }
   },
   "source": [
    "3. **center matrix: H** \\\n",
    "取<font color=blue>$H=(I_n-\\frac{1}{n}1_n1_n^T)$</font>，有以下性质：\\\n",
    "<font color=green>a. $\\begin{align}\n",
    "& S = \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}(x_i-\\bar x)(x_i-\\bar x)^{T} = \\frac{1}{n}X^THX\n",
    "\\end{align}$ \\\n",
    "b. $H =H^T, H =H^2=H^n$ \\\n",
    "c. $HX=X-\\bar X$，因此，H称为center matrix\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddaad76",
   "metadata": {},
   "source": [
    "## III. 最大投影方差\n",
    "1. **性质：当w是单位向量时，$x^Tw=|x|·cos\\theta$是x在w方向上的投影长度。** \\\n",
    "<font color=orange>证明： \\\n",
    "根据几何性质：x在w方向上的投影大小(长度)是：$|x|·cos\\theta$，其中$\\theta$是向量x的方向与w方向的夹角。 \\\n",
    "根据内积特征：$x^Tw=|x|·|w|·cos\\theta$，当$|w|=1$时，$x^Tw=|x|·cos\\theta$ </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13230e12",
   "metadata": {},
   "source": [
    "2. **定义**：n个向量${x_1, x_2, ..., x_n}$在$w_k$方向上的投影方差记为$J_k$，有：$$\n",
    "J_k =\\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}(x_i^Tw_k)^2 \n",
    "$$\n",
    "在K个方向上的投影方差之和为：$$\\begin{align} \n",
    "  J & =  {\\textstyle \\sum_{k=1}^{K}} J_k \\\\\n",
    "&= {\\textstyle \\sum_{k=1}^{K}}\\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}(x_i^Tw_k)^2 \\\\\n",
    "&=\\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}{\\textstyle \\sum_{k=1}^{K}}(x_i^Tw_k)^2\n",
    "\\end{align}$$<font color=red>注：投影可能为正，也可能为负，最小投影方差用投影的平方来定义更合理</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a203f5b3",
   "metadata": {},
   "source": [
    "3. 理解最大投影方差：<font color=blue>**向量长度平方的均值**</font> \\\n",
    "(1)当坐标系是标准坐标系时 \\\n",
    "n个<font color=red>**归一化后**</font>的p维向量${x_1, x_2, ..., x_n}$在标准坐标$e_k$方向上的投影方差为：$$\n",
    "J_k =\\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}(x_i^Te_k)^2= \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}x_{i,k}^2\n",
    "$$\n",
    "在所有p个维度上的投影方差之和是各个向量的L2 norm的均值，也就是向量长度平方的均值。<font color=red>**由于此时向量已经归一化，所以向量长度平方的均值就是样本总体的方差。**</font>$$\n",
    "  J =  {\\textstyle \\sum_{k=1}^{p}} J_k =\\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}{\\textstyle \\sum_{k=1}^{p}}x_{i,k}^2\n",
    "=\\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}\\left \\| x_i \\right \\| _2^2$$\n",
    "(2)一般化到任意<font color=blue>单位正交</font>坐标系W \\\n",
    "①向量$x_i$在新坐标系下的坐标为$\\tilde x_i=x_i^TW$，n个向量X在新坐标系下的坐标为$\\tilde X=XW$。$$\\begin{align} \n",
    "  J_p & = \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}{\\textstyle \\sum_{k=1}^{p}}(x_i^Tw_k)^2 \\\\\n",
    "&= \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}\\left \\| x_i^TW \\right \\| _2^2 ，\\because W是单位正交坐标系\\\\\n",
    "&=\\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}\\left \\| \\tilde x_i \\right \\| _2^2 \n",
    "\\end{align}$$\n",
    "在所有p个维度上的投影方差之和是各个向量在新坐标系下的<font color=blue>新坐标</font>的L2norm的均值，也就是新向量$\\tilde x_i$长度平方的均值。\\\n",
    "②如果只是投影到W中的K维空间，坐标方向是$W_K，且K<p$，则投影方差之和是各个向量在新的k维坐标系下的新坐标的L2 norm的均值，也是投影长度的平方的均值。\n",
    "$$\\begin{align} \n",
    "  J_K & = \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}{\\textstyle \\sum_{k=1}^{K}}(x_i^Tw_k)^2 \\\\\n",
    "&= \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}\\left \\| x_i^TW_K \\right \\| _2^2 \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f1e016",
   "metadata": {},
   "source": [
    "4. **计算最大投影方差**：先去中心化(归一化)后再计算，$x_i'=x_i-\\bar x$ \\\n",
    "将X投影到向量$w_k$上，投影方差为：\n",
    "$$\\begin{align} \n",
    "J_k & =\\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}(x_i^{'T}w_k)^2 \\\\\n",
    "& = \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}((x-\\bar x)^{T}w_k)^2 \\\\\n",
    "& = \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}((x-\\bar x)^{T}w_k)^T((x-\\bar x)^{T}w_k) \\\\\n",
    "& = \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}(w_k^{T}(x-\\bar x))((x-\\bar x)^{T}w_k) \\\\\n",
    "& = w_k^{T}(\\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}(x-\\bar x)(x-\\bar x)^{T})w_k \\\\\n",
    "& = w_k^{T}Sw_k \\\\\n",
    "\\\\\n",
    "\\end{align} $$\n",
    "如果将X投影到任意K个w向量为坐标构成的空间上，则投影方差合计：\n",
    "$$\\begin{align} \n",
    "J & =  {\\textstyle \\sum_{k=1}^{K}} J(w_k) = {\\textstyle \\sum_{k=1}^{K}}w_k^{T}Sw_k  \\\\\n",
    "\\end{align}$$\n",
    "<font color=red>说明：这里直接加总$x_i$在各个K个$w_k$上的投影，暗含的假设是：$w_i \\perp w_j，if\\ i\\ne j$。也就是说，目标函数中已经假设了找到的各个最小投影方差的方向是相互正交的。原因详见后文：可以直接假设新坐标中的基是单位正交基</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c540ca41",
   "metadata": {},
   "source": [
    "5. **求解产生最大投影方差的方向** \\\n",
    "思路: 转化为优化问题 \\\n",
    "$$\\begin{align} \n",
    "&   \\hat w = \\underset{W}{argmax}\\ J(W) =\\underset{w_1, ..., w_K}{argmax}\\  {\\textstyle \\sum_{k=1}^{K}} w_k^TSw_k \\\\\n",
    "&  s.t\\ \\ w_k^Tw_k= 1, w_k^Tw_j=0, \\ （k,j = 1, 2, ..., K,且k\\ne j）\n",
    "\\end{align}$$ \\\n",
    "$$\\begin{align} \n",
    "构建拉格朗日方程：& L(W, \\lambda ) = \\  {\\textstyle \\sum_{k=1}^{K}} w_k^TSw_k +  {\\textstyle \\sum_{k=1}^{K}} \\lambda_k(1- w_k^Tw_k) \\\\\n",
    " \\\\\n",
    "求解最优化：& \\frac{\\partial L}{\\partial w_k} = 2Sw_k-2\\lambda_k w_k = 0 \\\\\n",
    "即： & (S-\\lambda_k I)w_k  = 0 \\\\\n",
    "解必然满足：& \\lambda_k 是S的特征值，w_k是S的特征向量。\n",
    "\\end{align}$$ \\\n",
    "又，$$\n",
    "\\begin{align} \n",
    "& \\because S = {\\textstyle \\sum_{i=1}^{p}} \\lambda _ig_ig_i^T \\\\\n",
    "& \\therefore J(g_j) = g_jSg_j^T = \\lambda _j \\\\\n",
    "& \\therefore \\hat W = argmax\\ J(W) = (g_1, g_2, ..., g_K)，是S最大的K个特征值的特征向量  \\\\\n",
    "& 此时，J(w)=\\lambda_1 + \\lambda_2 + ...+ \\lambda_K\n",
    "\\end{align}$$\n",
    "**结论：** \\\n",
    "<font color=blue>a.假如原x是p维，降维就是从样本方差矩阵S的特征值$\\lambda_1, \\lambda_2, ..., \\lambda_p$中找最大的K个特征值，用他们对应的特征向量做为基，构建新的特征空间。\\\n",
    "b.向量$x_i^{'}$在新空间上的坐标是$x_i^{'T}W$，也就是$x_i^{'}$在各个基向量$g_k$上的投影长度。\\\n",
    "c.得到的投影向量有最大投影方差。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16c2b2b",
   "metadata": {},
   "source": [
    "## IV. 最小重构代价"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2617cdc1",
   "metadata": {},
   "source": [
    "1. **性质：w是单位向量时，$x^Tww=|x|·cos\\theta·w$是x在w方向上的投影。** \\\n",
    "<font color=orange>证明略：同前文</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8f0726",
   "metadata": {},
   "source": [
    "2. **坐标变换** \\\n",
    "假如x是任意p维向量，在以p个<font color=blue>**单位正交**</font>向量${u_1, u_2, ..., u_p}$为基的坐标系下，x可以表达为：$$x_i^{'} =  {\\textstyle \\sum_{k=1}^{p}}(x_i^Tu_k)u_k$$\n",
    "<font color=orange>eg: 标准坐标系是其中一个特例，此时$(u_1, u_2, ..., u_p)=(e_1, e_2, ..., e_p)$，代入上式有，$$x_i=x_i$$</font> \\\n",
    "<font color=red>**说明：为什么这里可以直接假设新坐标中的基是单位正交基，后面用它的子集来作为重构空间的基？**</font> \\\n",
    "<font color=green>a. p维向量x在其所处的p维空间中，不论该空间用什么样的基，x的分布本身不会变化，变化的只是x在基上的坐标。 \\\n",
    "b. 对x做降维的时候，寻找的本质上是一个降维后的空间，而不在乎所选择的基。无论是最小重构代价还是最大投影方差，都是针对空间而言的，而不是针对某个具体的基而言的。只是数学上需要用具体的基来表达。 \\\n",
    "c. 通常该空间是唯一的，但是作为空间坐标系的基有无数种，为运算方便，直接用单位正交基是合理的。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8fe5e5",
   "metadata": {},
   "source": [
    "3. **最小重构代价** \\\n",
    "如果$q<p$，在以${u_1, u_2, ..., u_q}$为基的q维空间中找向量$x_i$在该空间的投影$ \\hat x_i$，有$$ \\hat x_i ={\\textstyle \\sum_{k=1}^{q}}(x_i^Tu_k)u_k$$\n",
    "如果用$\\hat x_i$作为$x_i$的近似值，则两者的差异：$$\n",
    "J_i =\\left \\| x_i-\\hat x_i \\right \\| ^2$$\n",
    "如果先做归一化，则：$$\\begin{align} \n",
    "J_i & =\\left \\| x_i^{'}-\\hat {x_i^{'}} \\right \\| ^2 \\\\\n",
    "& = \\left \\| {\\textstyle \\sum_{k=1}^{p}}(x_i^{'T}u_k)u_k - {\\textstyle \\sum_{k=1}^{q}}(x_i^{'T}u_k)u_k \\right \\| ^2 \\\\\n",
    "& = \\left \\| {\\textstyle \\sum_{k=q+1}^{p}}(x_i^{'T}u_k)u_k \\right \\| ^2 \\\\\n",
    "& = ({\\textstyle \\sum_{k=q+1}^{p}}(x_i^{'T}u_k)u_k)^T({\\textstyle \\sum_{k=q+1}^{p}}(x_i^{'T}u_k)u_k)，\\because x_i^{'T}u_k是scalar \\\\\n",
    "& = {\\textstyle \\sum_{k=q+1}^{p}}(x_i^{'T}u_k)^2(u_k^Tu_k)，\\because u_k是单位正交基 \\\\\n",
    "& = {\\textstyle \\sum_{k=q+1}^{p}}(x_i^{'T}u_k)^2 \\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "对所有X中的n个向量${x_1, x_2, ..., X_n}$都用投影做估计，则合计的平均差异是：\\\n",
    "$$\\begin{align} \n",
    "J& = \\frac{1}{n} {\\textstyle \\sum_{i=1}^{n}}J_i \\\\\n",
    "& = \\frac{1}{n}  {\\textstyle \\sum_{i=1}^{n}}{\\textstyle \\sum_{k=q+1}^{p}}(x_i^{'T}u_k)^2 \\\\\n",
    "& = {\\textstyle \\sum_{k=q+1}^{p}}(\\frac{1}{n}  {\\textstyle \\sum_{i=1}^{n}}(x_i^{'T}u_k)^2) \\\\\n",
    "& = {\\textstyle \\sum_{k=q+1}^{p}} u_k^TSu_k\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744f9a83",
   "metadata": {},
   "source": [
    "4. **计算最小重构代价的方向** \\\n",
    "转化成最优化问题，取U=(u_{q+1}, u_{q+2}, ..., u_p)，有:\n",
    "$$\\begin{cases}\n",
    "\\hat U = argmin\\ J(U) = \\underset {u_{q+1}, ..., u_p}{argmin}\\ {\\textstyle \\sum_{k=q+1}^{p}} u_k^TSu_k\\\\\n",
    "s.t \\ \\ u_k^Tu_k=1, u_k^Tu_j=0, \\ (k, j=q+1, q+2, ...,p,且k\\ne j)\n",
    "\\end{cases}$$\n",
    "$$\\begin{align} \n",
    " \\\\\n",
    "构建拉格朗日方程：& L(U, \\lambda ) = \\  {\\textstyle \\sum_{k={q+1}}^{p}} u_k^TSu_k +  {\\textstyle \\sum_{k={q+1}}^{p}} \\lambda_k(1- u_k^Tu_k) \\\\\n",
    "求解最优化：& \\frac{\\partial L}{\\partial u_k} = 2Su_k-2\\lambda_k u_k = 0 \\\\\n",
    "即： & (S-\\lambda_k I)u_k  = 0 \\\\\n",
    "解必然满足：& \\lambda_k 是S的特征值，w_k是S的特征向量。\n",
    "\\end{align}$$ \n",
    "又，$$\\begin{align} \n",
    "& \\because S = {\\textstyle \\sum_{i=1}^{p}} \\lambda _ig_ig_i^T \\\\\n",
    "& \\therefore J(g_j) = g_jSg_j^T = \\lambda _j \\\\\n",
    "& \\therefore \\hat U = argmax\\ J(U) = (g_{q+1}, g_{q+2}, ..., g_p)，是S最小的K个特征值的特征向量  \\\\\n",
    "& 此时，J(U)=\\lambda_{q+1} + \\lambda_{q+2} + ...+ \\lambda_p\n",
    "\\end{align}\n",
    "$$\n",
    "**结论：** \\\n",
    "<font color=blue>a.假如原x是p维，降维就是从样本方差矩阵S的特征值$\\lambda_1, \\lambda_2, ..., \\lambda_p$中**去掉最小的(p-q)个特征值，用剩余的q个特征值对应的特征向量做为基**，构建新的特征空间。与最大投影方差的结果完全一样。\\\n",
    "b.得到的投影向量有最小重构距离。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b6f0a0",
   "metadata": {},
   "source": [
    "## V. 用奇异值分解SVD代替谱分解提高运算效率\n",
    "1. 为什么要用奇异值分解代替谱分解？ \\\n",
    "a.奇异值分解：$(HX=u\\Sigma V^T)$ \\\n",
    "b.谱分解：$(S=Q\\Lambda Q^T)$ \\\n",
    "从计算复杂度来看，谱分解要先计算样本方差矩阵S，然后在计算谱分解。而奇异值分解可以不用计算样本方差，而直接通过SVD算法达到同样的效"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee5a1aa",
   "metadata": {},
   "source": [
    "2. 如何用奇异值分解实现谱分解，求解PCA \\\n",
    "分析：\\\n",
    "$$\n",
    "\\begin{align} \n",
    "& H_{n*n}X_{n*p}=U_{n*n}\\Sigma_{n*p}V_{p*p}^T \\begin{cases}\n",
    "U^TU=UU^T=I_n \\\\\n",
    "V^TV=VV^T=I_p \\\\\n",
    "\\Sigma 是对角矩阵(通常不是对称矩阵，即n\\ne q)\n",
    "\\end{cases}\\\\\n",
    "& S=X^THX = X^TH^THX= V\\Sigma^TU^TU\\Sigma V^T=V\\Sigma^2 V^T \\\\\n",
    "& 谱分解: S=G\\Lambda G^T，用V=G, \\Lambda=\\Sigma^2就可以得到谱分解结果\n",
    "\\end{align}$$\n",
    "<font color=green>PCA的压缩结果: \\\n",
    "S的特征向量是重构后的空间的基的方向，归一化后的$x_i^{'}$在新空间的坐标是：$x_i^{'T}V$。从整个样本上看，$X^{'}=X-\\bar X，其中\\bar X=1_{n}\\bar x^T$，在新空间的坐标是: $(X-\\bar X)V$。$$\\begin{align} \n",
    "& \\because X-\\bar X=HX \\\\\n",
    "& \\therefore (X-\\bar X)V=HXV=U\\Sigma V^TV=U\\Sigma \n",
    "\\end{align}$$ </font>\n",
    "<font color=blue>可见，用HX做奇异值分解得到的$\\Sigma, V, U$可以完成对S的分解，并得到样本X在用PCA完成维度压缩后的结果。优点是不用额外多做一次样本方差的计算。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a74bf5",
   "metadata": {},
   "source": [
    "## VI. PCoA\n",
    "1. 什么是PCoA \\\n",
    "与PCA不同，PCoA是取$B=HXX^TH^T$，对B做谱分解$B=G_B\\Lambda_B G_B^T$，得到B的特征值和特征向量。$$\n",
    "\\begin{align} \n",
    "& \\because B=HXX^TH^T= U\\Sigma V^TV\\Sigma ^TU^T=U\\Sigma^2U^T \\\\\n",
    "& \\therefore G_B=U, \\Lambda_B=\\Sigma^2 \\\\\n",
    "& \\therefore (X-\\bar X)V=U\\Sigma=G_B\\Lambda_B^{\\frac{1}{2}} \n",
    "\\end{align}$$\n",
    "结论：可以用谱分解的结果直接得到原向量在新空间上的投影结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dfe8fa",
   "metadata": {},
   "source": [
    "2. PCoA与PCA的区别 \\\n",
    "i.从效果上看，取B做谱分解与取S做谱分解的区别在于，S做出来得到的特征向量是新空间的基。而B做出来得到的$G_B和\\Lambda_B$简单运算后可以直接得到原向量在新空间上的投影结果。\\\n",
    "ii.从成本上看，S是$p*p$维，PCA对S做谱分解的复杂度是$O(p^3)$，PCoA对B做谱分解，B是$n*n$维，复杂度是$O(n^3)$。所以当n<p时，用PCoA的计算效率更高。\\\n",
    "iii.PCoA也有它对应的SVD分解，SVD分解的计算复杂度也更低。只是此时是对$(HX)^T$做SVD。分析过程和前面PCA一样（略）\\\n",
    "<font color=blue>iv.总的来说，SVD的复杂度更低，用reduced SVD可以达到时间和空间复杂度都是$O(np*min(n, p))$，也就是对于n和p中较大的那个的线性复杂度。n>>p时用HX做SVD；p>>n时用$(HX)^T$做SVD。</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
