{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d358a04b",
   "metadata": {},
   "source": [
    "# LGM: Factor analysis and PPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5590b3c",
   "metadata": {},
   "source": [
    "## Linear Gaussian Model\n",
    "#### 模型的目的和用途\n",
    "1. 目标：求$P(x)$\n",
    "2. 已知条件（假设）：x是由比它维度低的高斯随机变量加噪声生成的\n",
    "3. 应用场景：异常检测或样本生成\n",
    "#### 模型设定\n",
    "<font color=brown>[参考kevin Murphy2004ucb课程slide的定义]</font>\n",
    "1. <font color=blue>**定义：在LGM中，随机变量的条件概率分布是linear-Gaussian的，也就是说child = linear function of parent plus gaussian noise。**</font>\n",
    "2. **关键特征**：\n",
    "   - 假设\n",
    "        $$\\begin{align}\n",
    "        & x = \\Lambda z + \\epsilon ,\\ 这是linear的部分\\\\\n",
    "        & z\\sim N(\\mu_z, \\Sigma_z) ,\\ 这是Gaussian的部分\\\\\n",
    "        & \\epsilon \\sim N(\\mu_{\\epsilon }, \\Sigma_{\\epsilon}) ,\\ 这也是Gaussian的部分\\\\\n",
    "        & 注，不要求\\epsilon \\perp z，即噪声不一定与投影数据正交;但要求他们相互独立\n",
    "        \\end{align}$$\n",
    "   - 问题：求x的分布P(x)\n",
    "3. 不同的假设得到不同的具体模型\n",
    "   - 简单linear regression\n",
    "     - 如果z已知，也就是supervised model中，LGM就是简单线性回归模型\n",
    "   - <font color=blue>**Factor Analysis：**</font>\n",
    "     - 如果z未知，$\\mu_z=0,\\Sigma_z=I,\\mu_{\\epsilon}=0, \\Sigma_{\\epsilon}=\\psi, 且\\psi是对角矩阵$，此时的LGM称为Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5598f73b",
   "metadata": {},
   "source": [
    "## FA\n",
    "### I. 模型\n",
    "#### I.1 要解决的问题和模型提出的思想\n",
    "1. **第一类问题和思考**\n",
    "   - <font color=red>**问题**：在样本量n不大的时候估计高维(p维)随机变量x的分布P(x)。此时可以用分布P(x)来做generator或者异常检测。</font>\n",
    "   - <font color=green>**思考**：很多时候高维的x可能是由低维随机变量z通过线性投影变换到高维后加上高斯噪音生成的。</font>\n",
    "     - 其生成过程就表示为：先假设z有分布P(z)，对P(z)做抽样。再将抽样结果z用$\\Lambda$投影到更高维度的d维空间，然后加上高斯噪声$\\epsilon$的抽样，就可以得到P(x)分布的样本。\n",
    "     - 此时，得到的是生成模型：<font color=green>**a generator of the distribution of the training data.**</font>\n",
    "2. **第二类问题和思考**\n",
    "   - <font color=red>**问题**：想要用低维的随机变量z来解释高维样本之间的相关关系。</font>\n",
    "   - <font color=green>**思考**：高维随机变量x的样本互相之间有相似性，还有自己独立的特征。也许可以用低维度的随机变量z来表达相似性。剩余的部分就是每个样本独特的性质。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a49d98c",
   "metadata": {},
   "source": [
    "#### I.1 模型的假设条件\n",
    "1. 假设\n",
    "$$\n",
    "\\begin{align}\n",
    "线性：& x =\\mu +\\Lambda z + \\epsilon \\\\\n",
    "高斯：& \\epsilon \\sim N(0, \\Psi_d ) \\\\\n",
    "高斯：& z\\sim N(0, I_p) \\\\\n",
    "其他：& \\epsilon与z相互独立，注意不是 \\epsilon\\perp z\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed9bc94",
   "metadata": {},
   "source": [
    "2. 假设条件的含义\n",
    "   - **核心假设：**$\\epsilon \\sim N(0, \\Psi_d )$，这里限制了error covariance $\\Psi$为对角矩阵。再根据线性假设可以推出：$P(x|z) = N(\\mu+\\Lambda z, \\Psi)$。这意味着，在已知latent varible z的条件下，$x_i$之间是conditionally independent的。也因此，<font color=blue>这个假设是FA的核心假设。</font>\n",
    "     - <font color=deeppink>这里“条件独立”的说明：在给定z时，每个样本$x_i|z$都有相同的分布，他们的均值直接由$\\mu+\\Lambda z$给出，样本之间互不影响。且方差由$\\Psi$决定，也不存在互相的影响。因此，$P(x_i|z)=P(x_i|x_j,z)$，也就是任一样本的取值不影响其他样本。</font>\n",
    "     - 条件分布中，<font color=green>**z表达了样本之间相互关联的部分**</font>：共同的均值$\\mu+\\Lambda z$。从几何上看，这些样本投在用$\\Lambda$决定的超平面上的同一坐标z上。\n",
    "     - <font color=green>**$\\epsilon$则表达了每个样本unique的那部分信息**</font>。\n",
    "   - $z\\sim N(0, I)$，可见z中的scalar变量之间相互独立。也因此z在刻画变量之间的相关性时，可以高效地提供信息\n",
    "   - $\\Lambda$称为factor loading matrix，但由于加上了前述的限制，$\\Lambda$中的列向量并不对应PCA中的principle components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c425c42",
   "metadata": {},
   "source": [
    "3. 求解思路：基于上述假设，可以得到$x\\sim N(\\mu, \\Lambda\\Lambda^T+\\Psi)$,可以直接写出分布函数，因此可以用MLE求解参数$\\mu_{p\\times 1},\\Lambda_{p\\times d}, \\Psi_{d\\times 1}$，从而得到显式的分布函数。\n",
    "   - 分析：因为$P(x|z) = N(\\mu+\\Lambda z, \\Psi)$是正太分布，P(z)也是正太分布，所以x与z的联合分布P(x,z)也是正太分布。而正太分布的边际分布也是正太分布，所以P(x)是正太分布。\n",
    "    $$\\begin{align}\n",
    "    E(x) \n",
    "    & = E(\\mu+\\Lambda z+\\epsilon ) \\\\\n",
    "    & =\\mu+\\Lambda Ez+E\\epsilon \\\\\n",
    "    & =\\mu \\\\\n",
    "    Cov(x) \n",
    "    & = E(x-\\mu)(x-\\mu )^T \\\\\n",
    "    & =E(\\Lambda z+\\epsilon)(\\Lambda z+\\epsilon)^T \\\\\n",
    "    & =\\Lambda\\Lambda^T+\\Psi\\\\\n",
    "    \\therefore x & \\sim N(\\mu, \\Lambda\\Lambda^T+\\Psi)\n",
    "    \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5f1a9",
   "metadata": {},
   "source": [
    "#### I.4 <font color=green>LGM类模型中FA, PPCA和PCA的区别和联系</font>\n",
    "1. <font color=norange>**FA**</font>\n",
    "   - FA本质上是一个受限制的LGM。它的假设条件中的核心假设是噪音的协方差矩阵$\\Psi$是对角矩阵，此时噪音的各个维度之间相互独立，每个维度上的方差大小不一。\n",
    "   <img src=\"../pics/fa.png\" alt=\"alt text\" width=\"600\"/>\n",
    "   - <font color=green>**理解\"FA is just a constrained Gaussian.\"**</font>\n",
    "     - 假如x本身是一个普通的p维高斯分布，那么均值很容易估计，方差矩阵也可以用样本协方差矩阵估计。这是原始分布，没有任何的降维和重表达。FA的目的是用两个简单的分布来表达x本身的分布特征。对应$z\\sim N(0,1)$,$\\epsilon \\sim N(0,\\Psi)$。假如$\\Psi$不是diagonal matrix而是普通的$(p\\times p)$ matrix，那就失去了重表达的意义，因为$\\epsilon$本身的分布就已经和x的原分布一样复杂了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb6e02",
   "metadata": {},
   "source": [
    "2. <font color=norange>**PPCA**</font>\n",
    "   - 如果在FA的基础上进一步约束$\\Psi=\\sigma^2I$，就得到了<font color=blue>**PPCA模型**</font>。\n",
    "     - PPCA同样也是一个受限制的LGM，其受限制的程度比FA更强。因为对噪音$\\epsilon$的假设不只是各个维度相互独立，还限制了每个维度上的方差大小相等，都是$\\sigma^2$。\n",
    "     - 此时$\\Lambda$的列向量是principle components（通常是正交的）\n",
    "   - 对比FA和PPCA\n",
    "     - 高斯分布是超椭圆体，均值就是椭圆体的中心点，样本协方差矩阵的eigenvector就是椭圆体的轴方向，样本协方差矩阵的eigenvalue对应椭圆体在各个方向上的轴长。\n",
    "     <img src=\"../pics/fa_ppca.png\" alt=\"alt text\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de0f822",
   "metadata": {},
   "source": [
    "3. <font color=norange>**PCA**</font>\n",
    "   - PCA可以视为PPCA中$\\sigma^2\\rightarrow0$的特殊情况。<font color=green>[详见后文]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ae7b7c",
   "metadata": {},
   "source": [
    "### II. 最优化问题求解\n",
    "1. 思路\n",
    "   - 已知x的n个样本，做MLE，同时模型中有latent variable z，因此典型解法是用EM算法。\n",
    "     - E step: $用当前参数求Q(z)=P(z|x)$\n",
    "     - M step: 代入Q(z)后计算$\\underset {\\Lambda, \\mu, \\Psi}{argmax}ELBO$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95107871",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6172f11",
   "metadata": {},
   "source": [
    "2. 计算$P(z|x)$\n",
    "$$\\begin{align}\n",
    "cov(x, z)\n",
    "& = E(x-Ex)(z-Ez)^T \\\\\n",
    "& = E(x-\\mu)(z-0)^T \\\\\n",
    "& = E(\\Lambda z+\\epsilon )z^T \\\\\n",
    "& = E(\\Lambda zz^T+\\epsilon z^T) \\\\\n",
    "& = \\Lambda E(zz^T)+E(\\epsilon z^T) \\\\\n",
    "& = \\Lambda I+0=\\Lambda \\\\\n",
    "由前面的条件可知：\n",
    "p(\\begin{bmatrix} z \\\\ x \\end{bmatrix})\n",
    "& = N(\\begin{bmatrix} 0 \\\\\\mu \\end{bmatrix}|\\begin{bmatrix}\n",
    " I & \\Lambda^T \\\\ \n",
    " \\Lambda & \\Lambda\\Lambda^T + \\Psi\n",
    "\\end{bmatrix}) \\\\\n",
    "如果样本x是中心化的，则\\mu=0，此时：\\\\\n",
    "p(\\begin{bmatrix} z \\\\ x \\end{bmatrix})\n",
    "& = N(\\begin{bmatrix} 0 \\\\0 \\end{bmatrix}|\\begin{bmatrix}\n",
    " I & \\Lambda^T \\\\ \n",
    " \\Lambda & \\Lambda\\Lambda^T + \\Psi\n",
    "\\end{bmatrix})\n",
    "\\end{align}$$\n",
    "用联合高斯分布可以求解条件概率分布\n",
    "$$\n",
    "\\begin{align}\n",
    "P(z|x) & =N(\\mu_{z|x}, \\Sigma_{z|x}) \\\\\n",
    "其中：\n",
    "\\mu_{z|x} \n",
    "& = \\Lambda^T(\\Lambda\\Lambda^T+\\Psi)^{-1}(x-\\mu )\\\\\n",
    "中心化后有：\n",
    "\\mu_{z|x} & = \\Lambda^T(\\Lambda\\Lambda^T+\\Psi)^{-1}x \\\\\n",
    "\\Sigma_{z|x} & = I_p-\\Lambda^T(\\Lambda\\Lambda^T+\\Psi)^{-1}\\Lambda\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a6099f",
   "metadata": {},
   "source": [
    "3. 计算定Q(z)的取值时,ELBO的表达式\n",
    "   $$\\begin{align}\n",
    "\\underset {\\Lambda, \\Psi}{argmax}\\Sigma_ilogP(x_i)\n",
    "& =\\underset {\\Lambda, \\Psi}{argmax}\\Sigma_ilog\\int_zP(x_i|z)P(z)dz \\\\\n",
    "& =\\underset {\\Lambda, \\Psi}{argmax}\\Sigma_ilog\\int_zQ(z)\\frac{P(x_i|z)P(z)}{Q(z)}dz \\\\\n",
    "& \\ge \\underset {\\Lambda, \\Psi}{argmax}\\Sigma_i\\int_zQ(z)log\\frac{P(x_i|z)P(z)}{Q(z)}dz \\\\\n",
    "& = \\underset {\\Lambda, \\Psi}{argmax}ELBO(Q,\\Lambda, \\Psi)\n",
    "\\end{align}$$\n",
    "   - 给定Q(z)的取值时，最大化ELBO目标可以表示为：\n",
    "$$\\begin{align}\n",
    "\\underset {\\Lambda, \\Psi}{argmax}ELBO(\\bar Q;\\Lambda, \\Psi)\n",
    "& = \\underset {\\Lambda, \\Psi}{argmax}\\Sigma_i\\int_z\\bar Q(z)log\\frac{P(x_i|z)P(z)}{\\bar Q(z)}dz \\\\\n",
    "& = \\underset {\\Lambda, \\Psi}{argmax}\\Sigma_i\\int_z\\bar Q(z)[logP(x_i|z)+logP(z)-log\\bar Q(z)]dz \\\\\n",
    "& = \\underset {\\Lambda, \\Psi}{argmax} \\Sigma_i\\int_z\\bar Q(z)logP(x_i|z)dz， \\because P(z)和Q(z)不受参数影响 \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bcf764",
   "metadata": {},
   "source": [
    "4. 直接代入$P(x|z) = N(\\mu+\\Lambda z, \\Psi)$，用$\\frac{\\partial ELBO}{\\partial \\Lambda}=0$和$\\frac{\\partial ELBO}{\\partial \\Psi}=0$解参数可得：\n",
    "$$\\begin{align}\n",
    "\\Lambda & =\\left( \\Sigma_i \\int_z \\bar Q(z)x_iz^Tdz\\right)\\left( \\Sigma_i \\int_z \\bar Q(z)zz^Tdz\\right)^{-1}\\\\\n",
    "\\Psi & = diag\\frac{1}{N}\\left( \\Sigma_ix_ix_i^T-\\Lambda\\Sigma_i\\int_z \\bar Q(z)x_iz^Tdz \\right)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e064e",
   "metadata": {},
   "source": [
    "5. 解的特征：$\\Lambda$的解不唯一\n",
    "     - 说明：假设R是单位正交矩阵，即$RR^T=I$，那么取$\\tilde \\Lambda=\\Lambda R$，代入前面的推导过程，会发现$\\tilde \\Lambda$对模型没有影响\n",
    "       1. x的分布没有变化：因为唯一受影响的$Cov(x)= \\Lambda R(\\Lambda R)^T+\\Psi = \\Lambda\\Lambda^T+\\Psi$实际上没有变化\n",
    "       2. 从含以上说，$x=\\Lambda Rz+\\epsilon$实际上没有改变x被投影的位置，只是改变了坐标，此时z的取值会发生抵消性变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29fe7d1",
   "metadata": {},
   "source": [
    "### III. LGM模型的限制 \n",
    "- 用Linear Gaussian Model解决生成任务时，有一个核心假设：$x = \\Lambda z + \\epsilon$，也就是说，样本随机变量是与latent varible之间是线性关系。\n",
    "  - 如下图所示：LGM可以表示上图的关系，但不能表示下面那张图。这种情况下，LGM的效果不好。\n",
    "  <img src=\"../pics/fa_1D.png\" alt=\"alt text\" width=\"600\"/>\n",
    "  <img src=\"../pics/LGM_limitation.png\" alt=\"alt text\" width=\"600\"/>\n",
    "- 解决方式：用non-linear Gaussian Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7e7d6e",
   "metadata": {},
   "source": [
    "## PPCA\n",
    "### I. 模型\n",
    "#### I.1 模型的假设条件\n",
    "$$\n",
    "\\begin{align}\n",
    "& x =\\Lambda z + \\epsilon，这里假设x已经中心化 \\\\\n",
    "& z\\sim N(0, I_p) \\\\\n",
    "& \\epsilon \\sim N(0, \\sigma^2I ) \\\\\n",
    "& \\epsilon与z相互独立，\\epsilon中的每个元素独立同分布，方差都是\\sigma^2\n",
    "\\end{align}\n",
    "$$\n",
    "  - $\\Lambda$称为factor loading matrix\n",
    "  - 参数$\\Lambda_{p\\times d}, \\sigma$都未知，待求解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9a1b96",
   "metadata": {},
   "source": [
    "#### I.2 分析\n",
    "   - P(x)的分布\n",
    "     - x的边际分布：因为$P(x|z) = N(\\mu+\\Lambda z, \\sigma^2I)$是正太分布，P(z)也是正太分布，所以x与z的联合分布P(x,z)也是正太分布。而正太分布的边际分布也是正太分布，所以P(x)是正太分布。\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    E(x) \n",
    "    & = E(\\mu+\\Lambda z+\\epsilon ) \\\\\n",
    "    & =\\mu+\\Lambda Ez+E\\epsilon \\\\\n",
    "    & =\\mu \\\\\n",
    "    Cov(x) \n",
    "    & = E(x-\\mu)(x-\\mu )^T \\\\\n",
    "    & =E(\\Lambda z+\\epsilon)(\\Lambda z+\\epsilon)^T \\\\\n",
    "    & =\\Lambda\\Lambda^T+\\sigma^2I\\\\\n",
    "    \\therefore x & \\sim N(\\mu, \\Lambda\\Lambda^T+\\sigma^2I)\n",
    "    \\end{align}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69d95a2",
   "metadata": {},
   "source": [
    "### II. 最优化问题求解\n",
    "- 此时既可以用EM算法，也可以直接求解.EM算法和FA中一样，计算过程会更简单。[参考PRML p574]\n",
    "#### II.1 直接求解简单结论\n",
    "<font color=green>[参考paper：Probabilistic Principal Component Analysis, Tipping and Bishop]</font>\n",
    "  1. 后验分布：$$\\begin{align}\n",
    "& P(z|x)\\sim N(\\mu_{z|x}, \\Sigma_{z|x}) \\\\\n",
    "& \\mu_{z|x}=(\\Lambda^T\\Lambda+\\sigma^2I)^{-1}\\Lambda^T(x-\\mu) \\\\\n",
    "& \\Sigma_{z|x} = \\sigma^-2(\\Lambda^T\\Lambda+\\sigma^2I)\n",
    "\\end{align}$$\n",
    "  2. 直接求解MLE参数估计结果：$$\\begin{align}\n",
    "& \\sigma^2 = \\frac{1}{d-q}\\Sigma_{j=q+1}^{d}\\lambda _j  \\\\\n",
    "& \\Lambda = U_q(\\Phi_q-\\sigma ^2I)^{\\frac{1}{2} }R \\\\\n",
    "& x的covariance\\ matrix记为S，\\lambda 是S的特征值，\\Phi_q是\\lambda为对角线元素的对角矩阵;\\\\\n",
    "& u是S的特征向量，U_q是与最大的q个\\lambda 对应的u为列向量的矩阵;\\\\\n",
    "& R是任意的单位正交矩阵\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88935184",
   "metadata": {},
   "source": [
    "#### II.2 估计结果的说明\n",
    "1. $\\Lambda z$与PCA中principle subspace投影的关系\n",
    "   - 如果取R=I，则$\\Lambda = U_q(\\Phi_q-\\sigma ^2I)^{\\frac{1}{2} }$。PPCA中的变换$\\Lambda z$等价于：\n",
    "     - 先将latent varible用diagonal matrix$(\\Phi_q-\\sigma ^2I)^{\\frac{1}{2}}$做伸缩得到$z^{'}=(\\Phi_q-\\sigma ^2I)^{\\frac{1}{2}}z$\n",
    "     - 再将$z^{'}$投到principle subspace$U_q$上。因为$U_q$就是PCA中的principle subspace。\n",
    "     - 尽管$\\Lambda z$仍然位于和PCA中一样的subspace上，但是由于$\\epsilon\\perp z$不成立，所以x在该subspace上的投影并不满足最小重构代价，所以具体的投影位置和PCA中并不相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa99d74",
   "metadata": {},
   "source": [
    "2. PCA是PPCA取$\\sigma^2\\rightarrow 0$的特殊模型\n",
    "   - 分析：\n",
    "     $$\\begin{align}\n",
    "& P(z|x)\\sim N(\\mu_{z|x}, \\Sigma_{z|x}) \\\\\n",
    "& \\mu_{z|x}=(\\Lambda^T\\Lambda+\\sigma^2I)^{-1}\\Lambda^T(x-\\mu) \\\\\n",
    "& \\Sigma_{z|x} = \\sigma^2(\\Lambda^T\\Lambda+\\sigma^2I)^{-1} \\\\\n",
    "当\\sigma^2 \\rightarrow 0 时：\\\\\n",
    "& \\Sigma_{z|x} =0 \\\\\n",
    "& \\mu_{z|x}=(\\Lambda^T\\Lambda)^{-1}\\Lambda^T(x-\\mu),注\\ne \\Lambda^{-1}(x-\\mu )，因为\\Lambda不可逆\\\\\n",
    "代入MLE结果：\\\\\n",
    "& \\mu_{z|x}=\\Phi_q^{-\\frac{1}{2}}U_q^T(x-\\mu )\n",
    "\\end{align}$$\n",
    "   - 结论：<font color=blue>**中心化后的$x-\\mu$被$U_q$投影到了对应的正交空间上，这和PCA中一样。**</font>和PCA中的差异是，投影后的坐标的均值要经过$\\Phi_q$做伸缩才能得到。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90af904a",
   "metadata": {},
   "source": [
    "## 附\n",
    "### 1. 解FA的最优化问题\n",
    "$$\n",
    "\\begin{align}\n",
    " \\because P(x|z) &= N(\\mu+\\Lambda z, \\Psi) = N(\\Lambda z, \\Psi)\\\\\n",
    " \\therefore P(x_i|z) &=\\frac{exp(-\\frac{1}{2}(x_i-\\Lambda z)^T\\Psi ^{-1}(x_i-\\Lambda z))}{(2\\pi )^{\\frac{p}{2} }|\\Psi|^{\\frac{1}{2}}} \\\\\n",
    " logP(x_i|z) & = -\\frac{1}{2}(x_i-\\Lambda z)^T\\Psi ^{-1}(x_i-\\Lambda z) - \\frac{p}{2}log(2\\pi) -\\frac{1}{2}log|\\Psi| \\\\\n",
    "\\underset{\\Lambda,\\Psi}{argmax} ELBO \n",
    "& = \\underset{\\Lambda,\\Psi}{argmax} \\Sigma_i \\int_z \\bar Q(z)logP(x_i|z)dz \\\\\n",
    "& = \\underset{\\Lambda,\\Psi}{argmax} \\Sigma_i \\int_z \\bar Q(z)\\left( -\\frac{1}{2}(x_i-\\Lambda z)^T\\Psi ^{-1}(x_i-\\Lambda z)-\\frac{1}{2}log|\\Psi|\\right )dz \\\\\n",
    "由\\frac{\\partial ELBO}{\\partial \\Lambda} = 0可得：&\\\\\n",
    "&\\Leftrightarrow 0 = \\nabla_{\\Lambda}\\Sigma_i \\int_z \\bar Q(z)\\left[ -\\frac{1}{2}(x_i-\\Lambda z)^T\\Psi ^{-1}(x_i-\\Lambda z)\\right ]dz \\\\\n",
    "&\\Leftrightarrow 0 = \\Sigma_i \\int_z \\bar Q(z)\\nabla_{\\Lambda}[ (x_i-\\Lambda z)^T\\Psi ^{-1}(x_i-\\Lambda z)]dz \\\\\n",
    "&\\Leftrightarrow 0 = \\Sigma_i \\int_z \\bar Q(z)[2\\Psi ^{-1} (x_i-\\Lambda z)z^T]dz \\\\\n",
    "&\\Leftrightarrow 0 = \\Sigma_i \\int_z \\bar Q(z)[(x_i-\\Lambda z)z^T]dz \\\\\n",
    "&\\Rightarrow \\Lambda =\\left( \\Sigma_i \\int_z \\bar Q(z)x_iz^Tdz\\right)\\left( \\Sigma_i \\int_z \\bar Q(z)zz^Tdz\\right)^{-1}\\\\\n",
    "同样方式可以求得：&\\\\\n",
    "&\\Rightarrow \\Psi = diag\\frac{1}{N}\\left( \\Sigma_ix_ix_i^T-\\Lambda\\Sigma_i\\int_z \\bar Q(z)x_iz^Tdz \\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581bbdcf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
