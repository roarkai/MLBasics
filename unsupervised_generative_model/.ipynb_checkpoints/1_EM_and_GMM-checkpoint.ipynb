{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298fbf61",
   "metadata": {},
   "source": [
    "# Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe4f075",
   "metadata": {},
   "source": [
    "## I. 适用场景\n",
    "### I.1 有latent varible场景下的MLE问题\n",
    "$$\n",
    "\\begin{align}\n",
    "Likelihood: &\\\\\n",
    "& L(\\theta)  = \\Sigma_{i  = 1}^{N}logP(x_i;\\theta) \\\\\n",
    "原目标：&\\\\\n",
    "& \\underset{\\theta }{argmax}L=\\underset{\\theta }{argmax}\\Sigma_{i  = 1}^{N}logP(x_i;\\theta)\\\\\n",
    "考虑z后的目标：&z是latent\\ variable\\\\\n",
    "1.离散分布：& \\underset{\\theta }{argmax}\\Sigma_{i  = 1}^{N}log\\Sigma_zP(x_i, z;\\theta) \\\\\n",
    "2.连续分布：& \\underset{\\theta }{argmax}\\Sigma_{i  = 1}^{N}log\\int_{z}P(x_i, z;\\theta)dz \n",
    "\\end{align}\n",
    "$$\n",
    "### I.2 有incomplete data场景下的MLE问题\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10f554",
   "metadata": {},
   "source": [
    "## II. EM算法\n",
    "### II.1 单样本分析\n",
    "#### II.1.1 目标函数分析\n",
    "1. 目标函数\n",
    "$$logP(x_i;\\theta)=log\\Sigma_zP(x_i, z;\\theta)$$\n",
    "2. 函数性质:*[证明见附录]*\n",
    "   1. 目标函数有lower bound\n",
    "    $$\\begin{align}\n",
    "    logP(x_i;\\theta)\n",
    "    &=log\\Sigma_zP(x_i, z;\\theta) \\\\\n",
    "    &=log\\Sigma_zQ(z)*\\frac{P(x_i, z;\\theta)}{Q(z)}\\\\\n",
    "    &\\ge \\Sigma_zQ(z)*log\\frac{P(x_i, z;\\theta)}{Q(z)}...[1]\n",
    "    \\end{align}$$<font color=red>注：$Q(z)$是一个密度函数，满足pdf的定义条件：$Q(z)>0$和$\\Sigma_zQ(z)=1$。但不是z的真实pdf。</font>\n",
    "   2. 当且仅当$Q(z)=P(z|x;\\theta)$时，等式成立，即：$$\n",
    "\\begin{align}\n",
    "logP(x_i;\\theta) \n",
    "& = \\Sigma_zQ(z)*log\\frac{P(x_i, z;\\theta)}{Q(z)}...[2]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2762bb4",
   "metadata": {},
   "source": [
    "#### II.1.2 Evidence lower bound (ELBO)\n",
    "1. 将不等式右边记为<font color=blue>**evidence lower bound(ELBO)**</font>：\n",
    "$$\\begin{align}\n",
    "ELBO(x;Q,\\theta)& = \\Sigma_zQ(z)log\\frac{P(x, z;\\theta)}{Q(z)}\\\\\n",
    "logP(x_i;\\theta)& \\ge ELBO(x_i;Q,\\theta)\n",
    "\\end{align}$$\n",
    "2. lower bound的特征\n",
    "   - ELBO给出了$logP(x_i;\\theta)$的下限\n",
    "   - 因为式中z会被积分积掉，所以ELBO是$x,\\theta,Q$的函数。\n",
    "   - 如果给定x，$P(x_i;\\theta)是\\theta$的函数，而ELBO是$\\theta和Q$的函数。此时，如果Q取不同的分布，那么$P(x_i;\\theta)$与ELBO之间的距离(差异)会变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825ae131",
   "metadata": {},
   "source": [
    "#### II.1.3 优化算法\n",
    "1. **思路**：<font color=blue>不断增加ELBO的大小，通过让lower bound最大化的方式来最大化原函数。</font>\n",
    "   - ELBO是目标函数下限，已知x的条件下，给定$\\theta$时，取$Q(z)=P(z|x;\\theta)$则两者相等。那么：\n",
    "     - 当$\\theta=\\theta_t$时，只要取$Q_t=P(z|x;\\theta_t)$，则ELBO就能取到它的最大值，也就是$logP(x;\\theta_t)$ \n",
    "     - 接下来Q不变，取$\\theta_{t+1}=argmaxELBO(x;Q_t,\\theta)$，那么改变后的ELBO比前一时刻的ELBO更大，但却比此时的$logP(x;\\theta_{t+1})$小\n",
    "   - 上面两种变量调整方式都会让ELBO变大，如果迭代可以收敛，即在某个时刻，$Q和\\theta$都不再变化，那么此时会有$logP(x;\\theta_t)=ELBO(x;Q^{*},\\theta)$，$logP(x;\\theta_t)$此时也会取到local min。\n",
    "<img src=\"../pics/ELBO.png\" width=\"560\" height=\"360\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028d0557",
   "metadata": {},
   "source": [
    "2. **EM算法**\n",
    "    - 随机初始化$\\theta_0$\n",
    "    - 迭代：\n",
    "      - E step(第t步)：$Q_t(z)=P(z|x;\\theta_t)$ \n",
    "      - M step(第t+1步)：$\\theta_{t+1}=\\underset{\\theta}{argmax}ELBO(x;Q_t,\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb860dc6",
   "metadata": {},
   "source": [
    "3. **算法收敛性证明：收敛到局部最优解**\n",
    "   - 前面已经证明了$Q(z)=P(z|x;\\theta_t)$时，$logP(x;\\theta_t) \n",
    "= ELBO(x;Q,\\theta_t)$，只要再证明$logP(x;\\theta_t)\\ge logP(x;\\theta_{t+1}) $即可。\n",
    "   - 证明：\n",
    "$$\\begin{align}\n",
    "logP(x;\\theta_{t+1}) & \\ge ELBO(x;Q_t,\\theta_{t+1}) \\\\\n",
    "& \\ge ELBO(x;Q_t,\\theta_{t}) \\\\\n",
    "& = logP(x;\\theta_{t})\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8daef96",
   "metadata": {},
   "source": [
    "### II.2 多样本分析\n",
    "#### II.2.1 目标函数分析\n",
    "1. 目标函数\n",
    "$$\\Sigma_ilogP(x_i;\\theta)=\\Sigma_ilog\\Sigma_zP(x_i, z;\\theta)$$\n",
    "2. 函数性质:\n",
    "   1. lower bound\n",
    "    $$\\begin{align}\n",
    "\\Sigma_ilogP(x_i;\\theta)\n",
    "&=\\Sigma_ilog\\Sigma_{z_i}P(x_i, z_i;\\theta) \\\\\n",
    "&=\\Sigma_ilog\\Sigma_{z_i}Q(z_i)*\\frac{P(x_i, z_i;\\theta)}{Q(z_i)}\\\\\n",
    "&\\ge \\Sigma_i\\Sigma_{z_i}Q(z_i)*log\\frac{P(x_i, z_i;\\theta)}{Q(z_i)} \\\\\n",
    "& = \\Sigma_iELBO(x_i;Q,\\theta )\n",
    "\\end{align}$$\n",
    "   2. 当且仅当$Q(z_i)=P(z_i|x_i;\\theta)$时，等式成立，即：$$\n",
    "\\begin{align}\n",
    "\\Sigma_ilogP(x_i;\\theta)= \\Sigma_iELBO(x_i;Q,\\theta )\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55928f37",
   "metadata": {},
   "source": [
    "3. Q的特征\n",
    "   - 假如有n个样本，要注意$Q(z_1),Q(z_2), ..., Q(z_n) $不是同一个Q函数的n种不同取值\n",
    "   - 每个样本$(x_i,z_i)$都有自己的$Q_{z_i}(z_i)$，Q函数的类型数取决于latent varible z的特征。\n",
    "   - 比如：在具体model中x来自K个分类，但每个$x_i$所属类型未知，用latent varible $z_i$表示，z的边际分布是多项式分布，即$P(z=z_k)=\\phi_k, \\Sigma_{k=1}^{K}\\phi_k=1$。此时，Q就有K种函数形态，每种对应K种分类中的一类。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef92a5",
   "metadata": {},
   "source": [
    "#### II.2.2 EM两步迭代\n",
    "- 随机初始化$\\theta_0$\n",
    "- 迭代：\n",
    "  - <font color=green>**E step(第t步)：$for\\ i=1, 2, ..., n,取Q_{i_t}=P(z_i|x_i;\\theta_{t})$。**</font>\n",
    "    - 如果可以直接用分布函数，就直接代入求解。以z是离散分布为例，求解方法：  \n",
    "    $$\\begin{align}\n",
    "Q_i(z=k) = P(z=k|x_i) & = \\frac{P(x_i,z=k)}{P(x_i)}\\\\\n",
    "& = \\frac{P(x_i,z=k)}{\\Sigma_{j=1}^{K}P(x_i, z=j)} \\\\\n",
    "& = \\frac{P(x_i|z=k)P(z=k)}{\\Sigma_{j=1}^{K}P(x_i|z=j)P(z=j)} \\\\\n",
    "\\end{align}$$\n",
    "    - 如果不能直接用分布函数，可以用sampling P(z=k|x)的方式来得到对该分布的近似估计。\n",
    "  - <font color=green>**M step(第t+1步)：$$\\begin{align}\n",
    "\\theta_{t+1}\n",
    "& =\\underset{\\theta}{argmax}\\Sigma_i ELBO(x_i;Q_{i_t},\\theta) \\\\\n",
    "& =\\underset{\\theta}{argmax}\\Sigma_i\\Sigma_{z_i}Q_i(z_i)*log\\frac{P(x_i, z_i;\\theta)}{Q_i(z_i)} \n",
    "\\end{align}$$**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eeb999",
   "metadata": {},
   "source": [
    "### II.3 直觉上理解EM两步算法在做什么\n",
    "- 如果没有缺失的信息，比如上面model中z是已知的，那么做MLE很容易。\n",
    "  - 把样本中z值不同的样本分到一起，分出来K类，然后对每一类估计$P(x_i|z_k;\\theta)$的参数$\\theta$\n",
    "  - 再用每类的样本数量来估计$P(z_k;\\phi)$的参数$\\phi$\n",
    "- 但是有缺失信息的时候无法对样本分类，EM的intuition是：\n",
    "  - E step：\n",
    "    1. 用当前的参数信息来猜未知的z分布信息：P(z|x)\n",
    "       - 计算方法：$P(z=k|x)=\\frac{P(z=k,x)}{\\Sigma_z P(z,x)}=\\frac{P(x|z=k)P(z=k)}{\\Sigma_z P(x|z)P(z)}$\n",
    "    2. 用P(z|x)来填上缺失的信息\n",
    "       - 填的方法：把样本做改造，每个样本$x_i$被分成K个子样本，每个子样本都分配了1个类型值$z=k$。样本$x_i$发生的概率1也分给这K个子样本，每个概率是$P(z=k|x_i)$\n",
    "       - 原来的N个没有z信息的样本被分成了$K*N$个有z信息的样本。\n",
    "  - M step：像latent varible的模型那样，用$K*N$个有z信息的样本重估参数\n",
    "    - 此时，每个样本在count它的数量的时候，不再是1个子样本count1次，而是1个子样本count$P(z=k|x_i)$次。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f6e030",
   "metadata": {},
   "source": [
    "### II.4 理解ELBO的另一种角度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d573b94",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "$$\\begin{align}\n",
    "logP(x;\\theta) & =ELBO(x;Q,\\theta)+D_{KL}(Q(z)||P(z|x)) \\\\\n",
    "似然 & =ELBO + KL divergence \\\\\n",
    "等价于：\\\\\n",
    "ELBO(x;Q,\\theta) & =logP(x;\\theta) - D_{KL}(Q(z)||P(z|x))\n",
    "\\end{align}$$\n",
    "</font>\n",
    "[证明见附录]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d279c3d6",
   "metadata": {},
   "source": [
    "<font color=green>**变分推断中也应用了该性质，思路是：由于$logP(x;\\theta)$不受$Q(z)$选择的影响，所以选择$Q(z)$让ELBO最大化相当于尽可能让$D_{KL}$逼近于0，此时$Q(z)$逼近$P(z|x)$，也就实现了对$P(z|x)$的估计。**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dd342f",
   "metadata": {},
   "source": [
    "## III. 在模型中的典型应用\n",
    "### III.1 解Density Estimation for a Gaussian mixture(GMM)\n",
    "#### III.1.1 GMM问题\n",
    "1. 问题\n",
    "   - 已知n个样本的取值，$x\\in\\{x_1, x_2, ..., x_n\\}$，这些样本来自K种类型$z\\in\\{z_1, z_2, ..., z_K\\}$，但每个样本属于哪种类型并不知道\n",
    "   - 如果给出新的x，希望判断P(x)大小\n",
    "2. 应用场景：异常检测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a3242",
   "metadata": {},
   "source": [
    "#### III.1.2 model\n",
    "1. 模型设定：\n",
    "   - 变量：\n",
    "     - observed evidence: x\n",
    "     - latent variable: z\n",
    "   - 变量分布：\n",
    "     - $z \\sim multinomial(\\phi)$\n",
    "     - $x|z \\sim N(\\mu_z, \\Sigma_z)$\n",
    "2. 目标函数：MLE\n",
    "$$\\begin{align}\n",
    "logL(\\phi,\\mu,\\Sigma) & =\\Sigma_{i=1}^{n}P(x_i;\\phi,\\mu,\\Sigma) \\\\\n",
    "& = \\Sigma_{i=1}^{n}\\Sigma_zP(x_i,z;\\phi,\\mu,\\Sigma) \\\\\n",
    "& = \\Sigma_{i=1}^{n}\\Sigma_{k=1}^{K}P(x_i|z=k;\\mu,\\Sigma)P(z=k;\\phi) \\\\\n",
    "\\underset{\\phi,\\mu,\\Sigma}{argmax}\\ L &= \\underset{\\phi,\\mu,\\Sigma}{argmax} \\Sigma_{i=1}^{n}\\Sigma_{k=1}^{K}P(x_i|z=k;\\mu,\\Sigma)P(z=k;\\phi)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e226718",
   "metadata": {},
   "source": [
    "3. 优化算法：EM迭代\n",
    "   1. 初始化：$\\mu_0, \\Sigma_0, \\phi_0$\n",
    "   2. **E-step**：$for\\ i=1, 2, ..., n,取Q_{i_t}=P(z_i|x_i;\\theta_{t})$\n",
    "         $$\\begin{align}\n",
    "    & Q_i(z=k) = P(z=k|x_i) = \\frac{P(x_i|z=k)P(z=k)}{\\Sigma_{j=1}^{K}P(x_i|z=j)P(z=j)} \\\\\n",
    "    t时刻代入：& P(x_i|z=k) = \\frac{exp\\{-0.5(x_i-\\mu_{tk})^T\\Sigma_{tk}^{-1}(x_i-\\mu_{tk})\\}}{(2\\pi)^{\\frac{d}{2} }|\\Sigma_{tk}|^{\\frac{1}{2} }} \\\\\n",
    "    &P(z=k) = \\phi_{tk}\n",
    "    \\end{align}$$<font color=green>对每个$x_i$分别求$Q_i(z=k),k=1, 2, ..., K$\n",
    "   3. **M-step**：$\\theta_{t+1}=\\underset{\\theta}{argmax}\\Sigma_i ELBO(x_i;Q_{i_t},\\theta) $\n",
    "        $$\\begin{align}\n",
    "    分别求解：&\\\\\n",
    "    &\\frac{\\partial ELBO}{\\partial \\mu_k}=0  \\rightarrow  \\mu_{k,t+1}=\\frac{\\Sigma_{i=1}^{n}Q_i(z=k)x_i}{\\Sigma_{i=1}^{n}Q_i(z=k)}\\\\\n",
    "    &\\frac{\\partial ELBO}{\\partial \\Sigma_k}=0  \\rightarrow  \\Sigma_{k,t+1}=\\frac{\\Sigma_{i=1}^{n}Q_i(z=k)(x_i-\\mu_k)(x_i-\\mu_k)^T}{\\Sigma_{i=1}^{n}Q_i(z=k)}\\\\\n",
    "    &\\frac{\\partial ELBO}{\\partial \\phi_k}=0  \\rightarrow  \\phi_{k,t+1}=\\frac{1}{n}\\Sigma_{i=1}^{n}Q_i(z=k)\n",
    "    \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9250e4a",
   "metadata": {},
   "source": [
    "4. 将优化结果代入模型处理inference问题\n",
    "$$\n",
    "\\begin{align}\n",
    "P(x) & = \\Sigma_k P(x|z=k)P(z=k) \\\\\n",
    "& = \\Sigma_k N(\\mu_k, \\Sigma_k)\\phi_k\n",
    "\\end{align}\n",
    "$$\n",
    "如果P(x)计算得到的值小于某个阈值，就可以判断x是outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a254ec",
   "metadata": {},
   "source": [
    "### III.2 用EM算法解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1d62ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "100b8dcf",
   "metadata": {},
   "source": [
    "## 附：\n",
    "### I. Jensen's inequality\n",
    "当f(x)为凸函数，x为随机变量时有：$$Ef(x)\\ge f(Ex)$$\n",
    "等式在以下情况下成立：\n",
    "1. f(x)是linear function.\n",
    "2. X以概率1取常数\n",
    "3. 如果f(x)严格凸, 那么当且仅当X是常数时等式成立"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b381c",
   "metadata": {},
   "source": [
    "#### 1. 证明:\n",
    "<font color=blue>$$\\Sigma_zQ(z)logh(z)\\le log\\Sigma_zQ(z)h(z)$$</font>\n",
    "其中：\n",
    "1. z是随机变量，有k种取值\n",
    "2. Q(z)是z的概率密度函数，满足$\\Sigma_zQ(z)=1$\n",
    "3. h(z)与z是一对一双射，即当$z_i\\ne z_j$时，$h(z_i)\\ne h(z_j)$。此时，有$Q(z)=Q(h(z))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee058be",
   "metadata": {},
   "source": [
    "分析：\\\n",
    "f(x)=log(x)时，由于log是严格凹函数，所以反过来有：$$\n",
    "\\begin{align}\n",
    "E(log(x)) &\\le log(E(x)) \\\\\n",
    "\\Sigma_{x} P(x)log(x) &\\le log\\Sigma_{x}P(x)x \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8994e4",
   "metadata": {},
   "source": [
    "因为$Q(z)=Q(h(z))$，代入上式有：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Sigma_zQ(z)logh(z)\n",
    "& = \\Sigma_zQ(h(z))log(h(z))\\\\\n",
    "& = \\Sigma_{h(z)}Q(h(z))log(h(z))\\\\\n",
    "& = \\Sigma_HQ(H)logH \\\\\n",
    "& \\le log\\Sigma_HQ(H)H \\\\\n",
    "& = log\\Sigma_{h(z)}Q(h(z))h(z) \\\\\n",
    "& = log\\Sigma_{z}Q(z)h(z) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe6815",
   "metadata": {},
   "source": [
    "#### 2. 证明[1]式：\n",
    "$$\\begin{align}\n",
    "logP(x_i;\\theta)\n",
    "&=log\\Sigma_zP(x_i, z;\\theta) \\\\\n",
    "&=log\\Sigma_zQ(z)*\\frac{P(x_i, z;\\theta)}{Q(z)}\\\\\n",
    "&\\le \\Sigma_zQ(z)*log\\frac{P(x_i, z;\\theta)}{Q(z)}=ELBO\n",
    "\\end{align}$$\n",
    "由Jenson不等式有：\n",
    "$$\\Sigma_zQ(z)logh(z)\\le log\\Sigma_zQ(z)h(z)$$\n",
    "取函数$h(z)=\\frac{P(x_i, z;\\theta)}{Q(z)}$, 代入上式得：\n",
    "$$\n",
    "log\\Sigma_zQ(z)*\\frac{P(x_i, z;\\theta)}{Q(z)} \\le \\Sigma_zQ(z)*log\\frac{P(x_i, z;\\theta)}{Q(z)}\n",
    "$$\n",
    "\n",
    "<font color=red>这里$Q(z)$是一个密度函数，满足pdf的定义条件：$Q(z)>0$和$\\Sigma_zQ(z)=1$。但并非z的真实pdf函数。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0579b01f",
   "metadata": {},
   "source": [
    "#### 3. 证明：\n",
    "<font color=blue>如果f是凸函数，且h(z)是z的一对一双射，那么$$E_z[f(h(z))]\\ge f(E_z[h(z)])$$ \n",
    "和Jensen不等式中一样，如果f(·)严格凸，那么当且仅当h(z)相对z是常数时上式取等。</font>\\\n",
    "证明：[将上一段证明方式一般化，即可得证]\n",
    "$$\n",
    "\\begin{align}\n",
    "E_z[f(h(z))]=\\Sigma_zQ(z)f(h(z))\\\n",
    "& = \\Sigma_zQ(h(z))f(h(z))\\\\\n",
    "& = \\Sigma_{h(z)}Q(h(z))f(h(z))\\\\\n",
    "& 取H=h(z),\\\\\n",
    "& = \\Sigma_HQ(H)f(H) \\\\\n",
    "& \\ge f\\left(\\Sigma_HQ(H)H\\right) \\\\\n",
    "& = f(\\Sigma_{h(z)}Q(h(z))h(z)) \\\\\n",
    "& = f(\\Sigma_{z}Q(z)h(z))=f(E_z[h(z)]) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "取等条件证明略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901b0f42",
   "metadata": {},
   "source": [
    "#### 4. 证明[2]式：\n",
    "<font color=blue>当且仅当$Q(z)=P(z|x;\\theta)$时下面等式成立：$$\n",
    "\\begin{align}\n",
    "logP(x;\\theta) \n",
    "& = \\Sigma_zQ(z)*log\\frac{P(x, z;\\theta)}{Q(z)} \\\\\n",
    "& = ELBO(x;Q,\\theta)\n",
    "\\end{align}$$</font>\n",
    "证明：\\\n",
    "因为log函数严格凹，所以当且仅当h(z)相对z是常数时$E_z[log(h(z))]\\le log(E_z[h(z)])$取等。\n",
    "$$\n",
    "\\begin{align}\n",
    "\\because h(z)& =\\frac{P(x, z;\\theta)}{Q(z)}=c， c是相对z的常数\\\\\n",
    "\\therefore Q(z)& =\\frac{P(x, z;\\theta)}{c}\\\\\n",
    "\\because \\Sigma_zQ(z)&=1,\\  \\therefore \\Sigma_z \\frac{P(x, z;\\theta)}{c} = \\frac{\\Sigma_zP(x, z;\\theta )}{c}=1 \\\\\n",
    "\\therefore c&= \\Sigma_zP(x, z;\\theta)=P(x;\\theta ) \\\\\n",
    "\\therefore 此时Q(z)& =\\frac{P(x, z;\\theta)}{c}=\\frac{P(x, z;\\theta)}{P(x;\\theta )}=P(z|x;\\theta )\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae625aee",
   "metadata": {},
   "source": [
    "### II. ELBO\n",
    "#### 1. 证明：似然 = ELBO + KL divergence\n",
    "<font color=blue>\n",
    "$$\\begin{align}\n",
    "logP(x;\\theta) & =ELBO(x;Q,\\theta)+D_{KL}(Q(z)||P(z|x)) \\\\\n",
    "\\end{align}$$\n",
    "</font>\n",
    "证明：\n",
    "$$\\begin{align}\n",
    "logP(x;\\theta) \n",
    "& = logP(x, z;\\theta) - logP(z|x;\\theta) \\\\\n",
    "& = log\\frac{P(x, z;\\theta)}{Q(z)} -log\\frac{P(z|x;\\theta)}{Q(z)} \\\\\n",
    "等式两边相对z的分布Q(z)求期望：\\\\\n",
    "E_zlogP(x;\\theta)\n",
    "& = E_z\\left (log\\frac{P(x, z;\\theta)}{Q(z)} -log\\frac{P(z|x;\\theta)}{Q(z)} \\right )\\\\\n",
    "左边& =\\Sigma_zQ(z)logP(x;\\theta)= logP(x;\\theta)\\Sigma_zQ(z)=logP(x;\\theta)\\\\\n",
    "右边\n",
    "& =\\Sigma_zQ(z)\\left (log\\frac{P(x, z;\\theta)}{Q(z)} -log\\frac{P(z|x;\\theta)}{Q(z)} \\right )\\\\\n",
    "& = \\Sigma_zQ(z)log\\frac{P(x, z;\\theta)}{Q(z)} -\\Sigma_zQ(z)log\\frac{P(z|x;\\theta)}{Q(z)}\\\\\n",
    "& = ELBO(x;Q,\\theta) - D_{KL}(Q(z)||P(z|x))\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7758c4",
   "metadata": {},
   "source": [
    "<font color=blue></font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
