{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4bb50d",
   "metadata": {},
   "source": [
    "# NLGM: non-linear Gaussian Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66f4eb8",
   "metadata": {},
   "source": [
    "## I. 背景\n",
    "### I.1 LGM模型的限制 \n",
    "- 用Linear Gaussian Model解决生成任务时，有一个核心假设：$x = \\Lambda z + \\epsilon$，也就是说，样本随机变量是与latent varible之间是线性关系。\n",
    "  - 如下图所示：LGM可以表示上图的关系，但不能表示下面那张图。\n",
    "  <img src=\"../pics/fa_1D.png\" alt=\"alt text\" width=\"500\"/>\n",
    "  <img src=\"../pics/LGM_limitation.png\" alt=\"alt text\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41941cb",
   "metadata": {},
   "source": [
    "### I.2 解决思路\n",
    "1. 方法：\n",
    "   1. 放松LGM的线性假设，取$x = f(z) + \\epsilon$\n",
    "      - <img src=\"../pics/NLGM_generator.png\" alt=\"alt text\" width=\"500\"/>\n",
    "   2. 同时用neural network作为$f(z)$，这样就能learn到非常复杂的结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e764b0d",
   "metadata": {},
   "source": [
    "2. 如何生成与样本分布一样的数据\n",
    "   - 作为generator的工作方式：从标准高斯分布中抽z，然后用f(z)将z投影到non-linear manifold上，最后加上高斯噪声。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb56390",
   "metadata": {},
   "source": [
    "## NLGM (也就是VAE)\n",
    "### I. 模型\n",
    "#### I.1 模型的假设条件\n",
    "$$\n",
    "\\begin{align}\n",
    "& x =f(z;\\theta) + \\epsilon=f_{\\theta}(z) + \\epsilon \\, ，这里\\theta是神经网络的weights\\\\\n",
    "& z\\sim N(0, I_p) \\\\\n",
    "& \\epsilon \\sim N(0, \\Psi_d )，简化取\\Psi_d=\\sigma^2I_d\\\\\n",
    "& \\epsilon与z相互独立\n",
    "\\end{align}\n",
    "$$\n",
    "  - 要learn的参数：神经网络中的参数$W, \\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcb9e07",
   "metadata": {},
   "source": [
    "#### I.2 目标函数:MLE\n",
    "$$ \\begin{align}\n",
    "max \\, logL(\\theta) \n",
    "& = max \\Sigma_{i=1}^{n} logP(x_i) \\\\\n",
    "& = max \\Sigma_{i=1}^{n} log\\int_z P(x_i, z)dz \\\\\n",
    "& = max \\Sigma_{i=1}^{n} log\\int_z P(x_i| z)P(z)dz \\\\\n",
    "\\end{align}$$\n",
    "- 分析：\n",
    "  1. log sum形态的目标函数没办法直接解最优化问题。\n",
    "  2. 这里是由latent variable的MLE估计，所以考虑用EM算法求解最优化问题。\n",
    "#### I.3 一般EM算法的求解思路\n",
    "1. 思路\n",
    "   - 将最大化likelihood转化为最大化ELBO\n",
    "   $$logP(x_i;\\theta) = ELBO(x_i;Q,\\theta)+D_{KL}(Q(z|x_i)||P(z|x_i))$$\n",
    "   $$\\begin{align}\n",
    "& \\because D_{KL}(Q(z|x_i)||P(z|x_i))  \\ge 0 \\\\\n",
    "& \\therefore logP(x_i;\\theta)\n",
    "  \\ge ELBO(x_i;Q,\\theta)\n",
    "\\end{align}$$\n",
    "   - 替代优化目标：\n",
    "   $$max \\Sigma_iELBO(x_i;Q,\\theta)  = max \\Sigma_iE_{z\\sim Q(z)}log\\frac{P(x_i, z;\\theta)}{Q(z)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d763e71",
   "metadata": {},
   "source": [
    "2. 算法实现方式\n",
    "   - E step：\n",
    "     - 求latent variable的后验分布$P(z|x)$，补齐样本信息。\n",
    "     - 操作方式：\n",
    "       1. 对每个样本$x_i$，用$sample(P(z|x_i))$得到$对应的z_i$\n",
    "       2. 将$(x_i,z_i)$代入ELBO。得到估计值：$$\n",
    "       \\begin{align}\n",
    "\\Sigma_iELBO(x_i;Q,\\theta)  \n",
    "& = \\Sigma_iE_{z\\sim Q(z)}log\\frac{P(x_i, z;\\theta)}{Q(z)} \\\\\n",
    "& = \\Sigma_ilog\\frac{P(x_i, z_i;\\theta)}{Q(z_i|x_i)}\n",
    "\\end{align}\n",
    "       $$\n",
    "   - M step：\n",
    "     - 更新参数：$$\\begin{align}\n",
    "\\theta_{t+1} & = \\underset{\\theta}{argmax}\\Sigma_i ELBO(x_i;Q, \\theta )\\\\\n",
    "& = \\underset{\\theta}{argmax}\\Sigma_ilog\\frac{P(x_i, z_i;\\theta)}{Q(z_i|x_i)} \\\\\n",
    "& = \\underset{\\theta}{argmax}\\Sigma_ilogP(x_i, z_i;\\theta) \\, ，\\because 这里Q(z_i|x_i)固定，不随\\theta变化\\\\\n",
    "& = \\underset{\\theta}{argmax}\\Sigma_ilogP(x_i| z_i;\\theta) \\, ，\\because P(z_i)与\\theta无关\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74de23eb",
   "metadata": {},
   "source": [
    "3. NLGM问题：无法直接求解$P(z|x)$的显式解\n",
    "   $$\\begin{align}\n",
    "P(z|x) & = \\frac{P(x|z;\\theta )P(z)}{P(x)} \\\\\n",
    "& = \\frac{P(x|z;\\theta)P(z)}{\\int_zP(x|z)P(z)dz} \\\\\n",
    "& = \\frac{N(f_\\theta (z),\\Psi)N(0,I)}{\\int_z N(f_\\theta (z),\\Psi)N(0,I) dz}，没有显示解\n",
    "\\end{align} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7791896c",
   "metadata": {},
   "source": [
    "#### I.4 VAE中使用EM算法的方式\n",
    "1. 思路：仍然使用EM算法，只是在E step中由于无法直接计算$P(z|x)$，改用$Q(z|x)$来估计$P(z|x)$。取$Q(z|x)\\sim N(\\mu(x), \\Sigma(x))$，用neural network来learn参数$\\mu(x), \\Sigma(x)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e76e72f",
   "metadata": {},
   "source": [
    "2. 迭代过程描述：下图中左边是正常EM的迭代过程，右图是VAE的迭代路径。可见：\n",
    "   - 每次E step无法达到logP(x)的lower bound，但通过$Q(z|x)$逼近$P(z|x)$仍然会让ELBO的值有所增加.\n",
    "   - 之后的M step还是可以在给定Q的条件下，通过参数迭代使ELBO达到它的最大值\n",
    "   <img src=\"../pics/ELBO.png\" width=\"500\" height=\"360\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6090117",
   "metadata": {},
   "source": [
    "### II. 调整后的EM算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4a5503",
   "metadata": {},
   "source": [
    "#### II.1 E step: 估计$Q(z|x)\\sim N(\\mu(x), \\Sigma(x))$\n",
    "1. 方法：通过$\\underset{\\varphi}{argmin}\\, KL(Q_{\\varphi}(z|x)||P(z|x;\\theta))$来找逼近$P(z|x;\\theta)$的$Q(z|x)$。\n",
    "   - <font color=brown>参数说明\n",
    "     1. $\\varphi$表示用来learn $\\mu(x), \\Sigma(x)$的神经网络中的参数。\n",
    "     2. $\\theta$是当前步中$P(x|z)=N(f(z),\\Psi)$中的参数的代称，实际包括神经网络$f(z)$中的权重和$\\Psi$。\n",
    "        - 因为：$$\\begin{align}\n",
    "P(z|x) & =\\frac{P(x|z;\\theta )P(z)}{\\int_zP(x|z;\\theta)P(z)dz} = \\frac{N(f(z),\\Psi)N(0,I)}{\\int_zN(f(z),\\Psi)N(0,I)dz} \n",
    " \\end{align}$$\n",
    "       因此，$\\theta$也是影响$P(z|x)$取值的参数。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f028c",
   "metadata": {},
   "source": [
    "2. 求解$\\underset{\\varphi}{argmin}\\, D_{KL}(Q_{\\varphi}(z|x)||P_{\\theta}(z|x))$\n",
    "   - 根据$D_{KL}$的以下计算特征可知： *[证明见附录]*\n",
    "$$\\begin{align}\n",
    "D_{KL}(Q_{{\\color{red} \\varphi}}(z|x)||P(z|x;{\\color{blue} \\theta}))\n",
    "& = logP(x;{\\color{blue} \\theta} ) - ELBO(x; Q_{{\\color{red} \\varphi}}(z|x),{\\color{blue} \\theta}  ) \\\\\n",
    "& = logP(x;{\\color{blue} \\theta}) - E_{z\\sim Q_{{\\color{red} \\varphi}}(z|x)}log\\frac{P(x|z;{\\color{blue} \\theta} )P(z)}{Q_{{\\color{red}\\varphi}}(z|x)} \\\\\n",
    "\\end{align}$$\n",
    "     - **$\\varphi$不影响$P(x)$，因此最小化KL divergence的目标等价于最大化ELBO。**\n",
    "     - <font color=blue>**这与EM算法中M step的目标一样，只不过在M step中是通过调整参数$\\theta$来实现max ELBO，这里是调整参数$\\varphi$**</font>。<font color=green>**实际上这两个参数互相并不影响，都是各自直接影响ELBO的大小，因此可以合并求解这两个最优化问题。**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4d75d8",
   "metadata": {},
   "source": [
    "3. 求解转化后的目标:$\\underset{\\varphi}{argmax}\\, ELBO(x; Q_{\\varphi}(z|x),\\theta)$\n",
    "   - ELBO分解：\n",
    "     $$\\begin{align}\n",
    "& \\underset{\\color{red}{\\varphi}}{argmax}\\, ELBO(x; Q_{\\color{red}{\\varphi}}(z|x),\\theta) \\\\\n",
    "& = \\underset{\\color{red}{\\varphi}}{argmax}\\, E_{z\\sim Q_{\\color{red}{\\varphi}}(z|x)}log\\frac{P_{{\\color{green} \\theta} }(x|z)P(z)}{Q_{\\color{red}{\\varphi}}(z|x)} \\\\\n",
    "& = \\underset{\\color{red}{\\varphi}}{argmax}\\, \\int_zQ_{\\color{red}{\\varphi}}(z|x)log\\frac{P_{\\color{green}\\theta}(x|z)P(z)}{Q_{\\color{red}{\\varphi}}(z|x)}dz\\\\\n",
    "\\end{align}$$\n",
    "   - 从分解式中可以看出，$\\varphi和\\theta$影响的是ELBO中的不同组成部分。M step中也是以max ELBO为目标做参数$\\theta$的估计。这里$\\varphi和\\theta$的取值并不对彼此产生影响，因此可以将两个优化问题合并完成。\n",
    "   - <font color=green>**所以在E step中先不解最优化问题，将它与M step合并来解$\\varphi$和$\\theta$**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd999a",
   "metadata": {},
   "source": [
    "4. 解优化问题的方法：将$ELBO(x_i;\\varphi,\\theta)$设为loss function，用SGD方法求解\n",
    "   - **不同参数影响哪些分布的判断要注意**：\n",
    "     - 从上面ELBO的分解式中来看，$\\varphi$只影响$Q_{\\varphi}(z|x)$,$\\theta$只影响$P_{\\theta}(x|z)$，且他们对$P(z)$都没有影响。\n",
    "     - 如果式中每个分布都有显示表达式可以代入，那么没有问题。\n",
    "     - 如果要用抽样方式估计$E_{z\\sim Q_{\\color{red}{\\varphi}}(z|x)}log\\frac{P_{{\\color{green} \\theta} }(x|z)P(z)}{Q_{\\color{red}{\\varphi}}(z|x)}$就要注意：<font color=red>**此时样本z是从$Q_{\\varphi}(z|x)$中抽样得到的，因此式中所有z的取值都会受$\\varphi$的影响，包括$P(z)和P(x|z)$。**</font>\n",
    "   - [具体过程详见后文]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa6e028",
   "metadata": {},
   "source": [
    "#### II.2 M step：用E step中最优化的结果进行抽样，补齐$(x_i, z_i)$，代入解max ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be000da9",
   "metadata": {},
   "source": [
    "1. 第一步：用$Q(z|x)$完成抽样$z_i=Sample(Q(z|x_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d3190a",
   "metadata": {},
   "source": [
    "2. 第二步：更新参数，最大化ELBO\n",
    "$$\\begin{align}\n",
    "\\theta_{t+1}\n",
    "& = \\underset{\\theta}{argmax}\\Sigma_i ELBO(x_i;Q, \\theta )\\\\\n",
    "& = \\underset{\\theta}{argmax}\\Sigma_ilog\\frac{P(x_i, z;\\theta)}{Q(z|x_i)} \\, ，这里Q(z|x)不受\\theta影响\\\\\n",
    "& = \\underset{\\theta}{argmax}\\Sigma_ilogP(x_i|z_i;\\theta)+logP(z_i) \\, ，这里P(z)不受\\theta影响\\\\\n",
    "& = \\underset{\\theta}{argmax}\\Sigma_ilogP(x_i|z_i;\\theta) \\, ，代入P(x|z)=N(f(z),\\sigma^2I_d)\\\\\n",
    "& = \\underset{\\theta}{argmax}\\Sigma_i-\\frac{d}{2}log\\sigma^2-\\frac{1}{2\\sigma^2}\\left \\| x_i-f_{\\theta}(z_i) \\right \\|^2  \\\\ \n",
    "& = \\underset{\\theta}{argmin}\\Sigma_i dlog\\sigma^2+\\frac{1}{\\sigma^2}\\left \\| x_i-f_{\\theta}(z_i) \\right \\|^2 \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b9f764",
   "metadata": {},
   "source": [
    "#### II.3 合并优化目标\n",
    "1. 合并优化目标\n",
    "$$\\begin{align}\n",
    "& \\underset{{\\color{red}{\\varphi}},{\\color{Green} \\theta} }{argmax} \\,\\Sigma_iELBO(x; Q_{\\color{red}{\\varphi}}(z|x),{\\color{Green} \\theta} ) \\\\\n",
    "& =\\underset{{\\color{red}{\\varphi}},{\\color{Green} \\theta} }{argmax} \\,\\Sigma_i\\, E_{z\\sim Q_{\\color{red}{\\varphi}}(z|x)}log\\frac{P_{{\\color{green} \\theta} }(x|z)P(z)}{Q_{\\color{red}{\\varphi}}(z|x)}\\\\\n",
    "& = \\underset{{\\color{red}{\\varphi}},{\\color{Green} \\theta} }{argmax} \\,\\Sigma_i\\, \\int_zQ_{\\color{red}{\\varphi}}(z|x)log\\frac{P_{\\color{green}\\theta}(x|z)P(z)}{Q_{\\color{red}{\\varphi}}(z|x)}dz\\\\\n",
    "& = \\underset{{\\color{red}{\\varphi}},{\\color{Green} \\theta} }{argmax}\\,\\Sigma_i \\int_zQ_{\\color{red}{\\varphi}}(z|x)logP_{\\color{green}\\theta}(x|z) -\\Sigma_i\\,\\int_zQ_{\\color{red}{\\varphi}}(z|x)log\\frac{Q_{\\color{red}{\\varphi}}(z|x)}{P(z)}dz \\\\\n",
    "& = \\underset{{\\color{red}{\\varphi}},{\\color{Green} \\theta} }{argmax}\\,\\Sigma_i E_{z\\sim Q_{\\color{red}{\\varphi}}(z|x)}logP_{\\color{green}\\theta}(x|z) - \\Sigma_iD_{KL}(Q_{\\color{red}{\\varphi}}(z|x)||P(z)) \\\\\n",
    "\\end{align}$$\n",
    "   - 为方便构造Gradient Descent中的loss function，选择以下等价的目标表达式：\n",
    "$$\\begin{align}\n",
    "\\underset{\\varphi,\\theta}{argmin}\\, \\Sigma_i \\int_zQ_{\\color{red}{\\varphi}}(z|x)log\\frac{Q_{\\color{red}{\\varphi}}(z|x)}{P(z)}dz - \\Sigma_i\\,E_{z\\sim Q_{\\color{red}{\\varphi}}(z|x)}logP_{\\color{green}\\theta}(x|z) \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d384321",
   "metadata": {},
   "source": [
    "2. 第一项直接求解\n",
    "   - 根据两个正态分布之间的KL divergence公式: *[证明见附录]*\n",
    "     $$D_{KL}(p || q) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_q^{-1} \\Sigma_p) + (\\mu_p - \\mu_q)^T \\Sigma_q^{-1} (\\mu_p - \\mu_q) - k + \\log \\left(\\frac{|\\Sigma_q|}{|\\Sigma_p|}\\right) \\right]$$\n",
    "     - 代入$p=Q_{\\varphi}(z|x)，q=N(0,I)$:\n",
    "       $$\\begin{align}\n",
    "D_{KL}(Q || N(0,I)) \n",
    "& = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_q^{-1} \\Sigma_Q) + (\\mu_Q - \\mu_q)^T \\Sigma_q^{-1} (\\mu_Q - \\mu_q) - d + \\log \\left(\\frac{|\\Sigma_q|}{|\\Sigma_Q|}\\right) \\right] \\\\\n",
    "& = \\frac{1}{2} [ \\text{tr}(\\Sigma_{\\varphi}(x)) +\\mu_{\\varphi}(x)^T\\mu_{\\varphi}(x)- d - \\log |\\Sigma_{\\varphi}(x)|  ]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e246997e",
   "metadata": {},
   "source": [
    "3. $P(x|z)$是neural network，所以第二项无法得到显式表达式，只能用抽样方法估计\n",
    "   - 样本量很大时，每个$x_i$可以只sample1次，取1个$z_i$:\n",
    "     $$\\Sigma_iE_{z\\sim Q_{\\varphi}(z|x)}logP(x_i|z;\\theta) = \\Sigma_i logP(x_i|z_i;\\theta)$$\n",
    "   - 展开概率表达式有：\n",
    "     $$\\begin{align}\n",
    "logP(x_i|z_i;\\theta) = \n",
    "-\\frac{p}{2}log\\sigma^2-\\frac{1}{2\\sigma^2}\\left \\| x_i-f_{\\theta}(z_i) \\right \\|^2\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de2415",
   "metadata": {},
   "source": [
    "4. 代入可得优化目标:\n",
    "$$\\begin{align}\n",
    "\\underset{\\varphi,\\theta}{argmax} \\,\\Sigma_iELBO \n",
    "&= \\underset{\\varphi,\\theta}{argmin}\\, \\Sigma_i\\int_zQ_{\\color{red}{\\varphi}}(z|x_i)log\\frac{Q_{\\color{red}{\\varphi}}(z|x_i)}{P(z)}dz - \\Sigma_i\\,E_{z\\sim Q_{\\color{red}{\\varphi}}(z|x_i)}logP_{\\color{green}\\theta}(x_i|z) \\\\\n",
    "&= \\underset{\\varphi,\\theta}{argmin}\\, \\Sigma_i\\frac{1}{2} [ \\text{tr}(\\Sigma_{\\varphi}(x_i)) +\\mu_{\\varphi}(x_i)^T\\mu_{\\varphi}(x_i)- d - \\log |\\Sigma_{\\varphi}(x_i)|  ] \\\\\n",
    "& -(-\\frac{d}{2}log\\sigma^2-\\frac{1}{2\\sigma^2}\\left \\| x_i-f_{\\theta}(z_i) \\right \\|^2) \\\\\n",
    "&= \\underset{\\theta,\\varphi}{argmin}\\,\\Sigma_i \\text{tr}(\\Sigma_{\\varphi}(x_i)) +\\left \\| \\mu_{\\varphi}(x_i) \\right \\|^2 - \\log|\\Sigma_{\\varphi}(x_i)| + dlog\\sigma^2 + \\frac{1}{\\sigma^2}\\left \\| x_i-f_{\\theta}(z_i) \\right \\|^2\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f053c",
   "metadata": {},
   "source": [
    "#### II.4 更新后的迭代算法\n",
    "- 将E step和M step中的优化问题合并，重新排序后的算法\n",
    "  - 初始化参数\n",
    "  - **迭代：**\n",
    "    - <font color=blue>\n",
    "第t步：用当前$Q(z|x,\\varphi)$抽样，补齐所有样本信息为$(x_i, z_i)$</font>\n",
    "    - <font color=blue>\n",
    "第t+1步：用SGD方法同步更新所有参数。$$\\theta_{t+1}, \\varphi_{t+1}\n",
    "= \\underset{\\theta,\\varphi}{argmax}\\Sigma_i ELBO(x_i;Q_{\\varphi }(z|x), \\theta )$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2490f",
   "metadata": {},
   "source": [
    "### III. VAE的优化求解过程\n",
    "#### III.1 第一步，抽样$Q_{\\varphi}(z|x)=N(\\mu(x), \\Sigma(x);\\varphi)$\n",
    "- 直接抽样存在的问题\n",
    "  1. 抽样结果方差大\n",
    "  2. 后面用SGD求解优化问题的时候很难对“抽样”操作求梯度\n",
    "- <font color=green>**solution：用重参数技巧做抽样**</font>\n",
    "  - 方法：将$Q_{\\varphi}(z|x)=N(\\mu(x), \\Sigma(x);\\varphi)$转化成等价的另一个分布：$$\\begin{align}\n",
    "z & = \\mu_{\\varphi}(x) + \\left(\\Sigma_{\\varphi}(x)\\right)^{\\frac{1}{2}}\\varepsilon  \\\\\n",
    "\\varepsilon  & \\sim N(0,I_p) \\, ,这里p=dim(z)\n",
    "\\end{align}$$\n",
    "  - 效果：通过将随机性转移到$\\varepsilon$上，使得对原参数$\\varphi$的梯度求解过程中不再有随机性。并且也降低了抽样的方差。**[为什么能降低抽样方差没有说明？？？]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3cbc1c",
   "metadata": {},
   "source": [
    "#### III.2  第二步，更新参数$\\varphi$和$\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9455f90",
   "metadata": {},
   "source": [
    "1. **优化目标**\n",
    "   - 原目标\n",
    "$$\\begin{align}\n",
    "\\theta_{t+1} & = \\underset{\\theta}{argmin}\\,\\Sigma_i dlog\\sigma^2+\\frac{1}{\\sigma^2}\\left \\| x_i-f_{\\theta}(z_i) \\right \\|^2 \\\\\n",
    "\\varphi_{t+1} & = \\underset{\\varphi}{argmin}\\,\\Sigma_i \\text{tr}(\\Sigma_{\\varphi}(x_i)) +\\left \\| \\mu_{\\varphi}(x_i) \\right \\|^2  - \\log|\\Sigma_{\\varphi}(x_i)| + \\frac{1}{\\sigma^2}\\left \\| x_i-f_{\\theta}(z_i) \\right \\|^2\n",
    "\\end{align}$$\n",
    "   - 合并后的目标 \n",
    "   $$\\underset{\\theta,\\varphi}{argmin}\\,\\Sigma_i \\text{tr}(\\Sigma_{\\varphi}(x_i)) +\\left \\| \\mu_{\\varphi}(x_i) \\right \\|^2 - \\log|\\Sigma_{\\varphi}(x_i)| + dlog\\sigma^2 + \\frac{1}{\\sigma^2}\\left \\| x_i-f_{\\theta}(z_i) \\right \\|^2$$\n",
    "     - <font color=green>注意：$f_{\\theta}(z_i)$中的$z_i$是$Q_{\\varphi}(z|x)$抽样的结果，所以求梯度时要考虑这个影响。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6fc933",
   "metadata": {},
   "source": [
    "2. **用优化目标构造Loss function**\n",
    "$$L=\\Sigma_i \\text{tr}(\\Sigma_{\\varphi}(x_i)) +\\left \\| \\mu_{\\varphi}(x_i) \\right \\|^2 - \\log|\\Sigma_{\\varphi}(x_i)| + dlog\\sigma^2 + \\frac{1}{\\sigma^2}\\left \\| x_i-f_{\\theta}(z_i) \\right \\|^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab815e21",
   "metadata": {},
   "source": [
    "3. **求解用到的梯度公式**\n",
    "$$\\begin{align}\n",
    "\\nabla_{\\varphi} L &= \\Sigma_i \\nabla_{\\varphi}\\text{tr}(\\Sigma_{\\varphi}(x_i)) +\\left \\| \\mu_{\\varphi}(x_i) \\right \\|^2 - \\log|\\Sigma_{\\varphi}(x_i)|+ \\nabla_{z}\\frac{1}{\\sigma^2}\\left \\| x_i-f_{\\theta}(z_i) \\right \\|^2\\nabla_{\\varphi}z \\\\\n",
    "\\nabla_{\\theta} L &= \\Sigma_i \\nabla_{\\theta}\\left(dlog\\sigma^2 + \\frac{1}{\\sigma^2}\\left \\| x_i-f_{\\theta}(z_i) \\right \\|^2 \\right)\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249e94f3",
   "metadata": {},
   "source": [
    "### IV. Inference阶段\n",
    "- 生成(图片)的方式：\n",
    "   - step 1：用$z\\sim N(0,I)$抽一个z\n",
    "   - step 2：用$f_{\\theta}(z)$得到结果，通常inference中，这步不再加噪音\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d60149",
   "metadata": {},
   "source": [
    "### V. VAE模型小结\n",
    "#### V.1 模型特征\n",
    "1. 模型是encoder-decoder结构，Encoder是$z|x\\sim N(\\mu_{\\varphi}(x),\\Sigma_{\\varphi}(x))$的部分，Decoder是$x|z\\sim N(f_{\\theta}(z), \\sigma^2I_p)$的部分。\n",
    "   - 完成训练后，扔掉encoder的部分，保留decoder就是generator，用于完成inference任务。\n",
    "2. VAE本质上就是LGM的非线性扩展。由于引入neural network后不能获得后验分布的显式表达式，因此在LGM的基础上做了几点调整：\n",
    "   1. 引入了另一个多元高斯分布来逼近后验分布。\n",
    "   2. 为了让整个过程可以顺利求梯度，因此在抽样$(x_i,z_i)$的时候用了重参数技巧\n",
    "3. 同样因为后验分布没有显式表达式，P(x)也无法得到显式表达式，因此VAE能通过抽样来generate P(x)的样本，但是却不能在给定x的条件下，判断它的概率大小，因此不能做异常检测。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc7ae72",
   "metadata": {},
   "source": [
    "#### V.2 优化目标的含义\n",
    "$$\\begin{align}\n",
    "& \\underset{{\\color{red}{\\varphi}},{\\color{Green} \\theta} }{argmax} \\,\\Sigma_iELBO(x_i; Q_{\\color{red}{\\varphi}}(z|x),{\\color{Green} \\theta} ) \\\\\n",
    "& = \\underset{{\\color{red}{\\varphi}},{\\color{Green} \\theta} }{argmax}\\,\\Sigma_i \\underset{En/Decoder\\, target}{\\underbrace{E_{z\\sim Q_{\\color{red}{\\varphi}}(z|x_i)}logP_{\\color{green}\\theta}(x_i|z)} }  - \\Sigma_i \\underset{regularization}{\\underbrace{D_{KL}(Q_{\\color{red}{\\varphi}}(z|x_i)||P(z))}}   \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd119f9d",
   "metadata": {},
   "source": [
    "1. 目标中第一部分表达的是：\n",
    "     - 先用$Q(z|x)$得到样本$x_i$的latent variable$z_i$的取值，完成x信息的encoder。\n",
    "     - 再用$P(f(z))$计算$x_i$的生成概率，完成对z中信息的Decoder。\n",
    "     - $max E_{z\\sim Q_{\\color{red}{\\varphi}}(z|x_i)}logP_{\\color{green}\\theta}(x_i|z)$整个可以看做最大化$x_i$的reconstruction probability\n",
    "2. 第二部分让后验分布逼近先验分布，有两个作用：\n",
    "     - 一是regulator，防止encoder过拟合\n",
    "     - 二是让decoder作为generator工作的时候，从先验假设$P(z)$中得到的抽样尽可能逼近training set中样本群体的latent variable取值$Q_{\\varphi}(z|x_i)$。这样才能保证生成的图片与training set中的图片尽可能接近，这也是模型泛化能力的体现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ffed8",
   "metadata": {},
   "source": [
    "## 附录\n",
    "### 附1. 计算两个正态分布之间的KL divergence公式\n",
    "- 给定两个k维多元正态分布p和q：\n",
    "  - $ p(x) = \\mathcal{N}(\\mu_p, \\Sigma_p) $\n",
    "  - $ q(x) = \\mathcal{N}(\\mu_q, \\Sigma_q) $\n",
    "  - $D_{KL}(p || q) = \\int p(x) \\log \\frac{p(x)}{q(x)} \\, dx$\n",
    "- 他们之间的KL divergence可以用下面公式计算：\n",
    "$$D_{KL}(p || q) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_q^{-1} \\Sigma_p) + (\\mu_p - \\mu_q)^T \\Sigma_q^{-1} (\\mu_p - \\mu_q) - k + \\log \\left(\\frac{|\\Sigma_q|}{|\\Sigma_p|}\\right) \\right]$$\n",
    "- 证明："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27294d1",
   "metadata": {},
   "source": [
    "#### 已知：\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x) = \\frac{1}{(2\\pi)^{k/2} |\\Sigma_p|^{1/2}} \\exp\\left(-\\frac{1}{2}(x - \\mu_p)^T \\Sigma_p^{-1} (x - \\mu_p)\\right) \\\\\n",
    "q(x) = \\frac{1}{(2\\pi)^{k/2} |\\Sigma_q|^{1/2}} \\exp\\left(-\\frac{1}{2}(x - \\mu_q)^T \\Sigma_q^{-1} (x - \\mu_q)\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### 代入：\n",
    "$$D_{KL}(p || q) = \\int p(x) \\left( \\log p(x) - \\log q(x) \\right) \\, dx$$\n",
    "\n",
    "**计算logP(x)和logq(x)** ：\n",
    "$$\\log p(x) = -\\frac{k}{2} \\log(2\\pi) - \\frac{1}{2} \\log |\\Sigma_p| - \\frac{1}{2}(x - \\mu_p)^T \\Sigma_p^{-1} (x - \\mu_p)$$\n",
    "$$\\log q(x) = -\\frac{k}{2} \\log(2\\pi) - \\frac{1}{2} \\log |\\Sigma_q| - \\frac{1}{2}(x - \\mu_q)^T \\Sigma_q^{-1} (x - \\mu_q)$$\n",
    "\n",
    "**代入KL divergence公式** ：\n",
    "$$\\begin{align}\n",
    "D_{KL}(p || q) \n",
    "& = \\frac{1}{2}\\int p(x) \\left(-\\log |\\Sigma_p| -(x - \\mu_p)^T \\Sigma_p^{-1} (x - \\mu_p)  + \\log |\\Sigma_q| + (x - \\mu_q)^T \\Sigma_q^{-1} (x - \\mu_q) \\right) dx \\\\\n",
    "& = -\\frac{1}{2} \\int p(x) \\log |\\Sigma_p| \\, dx + \\frac{1}{2} \\int p(x) \\log |\\Sigma_q| \\, dx \\\\\n",
    "& - \\frac{1}{2} \\int p(x) (x - \\mu_p)^T \\Sigma_p^{-1} (x - \\mu_p) \\, dx \\\\\n",
    "& + \\frac{1}{2} \\int p(x) (x - \\mu_q)^T \\Sigma_q^{-1} (x - \\mu_q) \\, dx\n",
    "\\end{align}$$\n",
    "\n",
    "#### 分别计算上面的三个部分\n",
    "1. **第一部分**: \n",
    "Since:\n",
    "$$\\int p(x) \\, dx = 1$$\n",
    "Thus,\n",
    "$$-\\frac{1}{2} \\int p(x) \\log |\\Sigma_p| \\, dx = -\\frac{1}{2} \\log |\\Sigma_p|$$\n",
    "$$\\frac{1}{2} \\int p(x) \\log |\\Sigma_q| \\, dx = \\frac{1}{2} \\log |\\Sigma_q|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b781d4",
   "metadata": {},
   "source": [
    "2. **先求第三部分，第二部分是它的特殊情况**: 此时求期望所使用的x的分布是p(x):\n",
    "   - 结果：$$E[(x - \\mu_q)^T \\Sigma_q^{-1} (x - \\mu_q)]= tr(\\Sigma_q^{-1}\\Sigma_p) - (\\mu_p - \\mu_q)^T \\Sigma_q^{-1} (\\mu_p - \\mu_q)$$\n",
    "\n",
    "   - 分析：\n",
    "     - 第一步：展开目标求解式$$E[(x - \\mu_q)^T \\Sigma_q^{-1} (x - \\mu_q)] = E[x^T \\Sigma_q^{-1} x] - 2\\mu_q^T \\Sigma_q^{-1} E[x] + \\mu_q^T \\Sigma_q^{-1} \\mu_q$$\n",
    "\n",
    "     - 第二步：求$E[x^T \\Sigma_q^{-1} x]$\n",
    "       - 正态分布有两个性质：\n",
    "       1. $Exx^T= \\Sigma + \\mu \\mu^T$ [详见证明1]\n",
    "       2. $\n",
    "E[x^T \\Sigma_q^{-1} x] = \\text{tr}(\\Sigma_q^{-1} E[xx^T])$ [详见证明2]\n",
    "       - 将B代入A可得：$$\\begin{align}\n",
    "E[x^T \\Sigma_q^{-1} x] & = tr(\\Sigma_q^{-1}(\\Sigma_p + \\mu_p \\mu_p^T)) \\\\\n",
    "& = tr(\\Sigma_q^{-1}\\Sigma_p + \\Sigma_q^{-1}\\mu_p \\mu_p^T) \\\\\n",
    "& = tr(\\Sigma_q^{-1}\\Sigma_p) + tr(\\Sigma_q^{-1}\\mu_p \\mu_p^T) \\\\\n",
    "& = tr(\\Sigma_q^{-1}\\Sigma_p) + tr(\\mu_p^T\\Sigma_q^{-1}\\mu_p ) \\\\\n",
    "& \\because \\Sigma_q^{-1} is \\, a \\, scalar \\\\\n",
    "& = tr(\\Sigma_q^{-1}\\Sigma_p) + \\mu_p^T\\Sigma_q^{-1}\\mu_p \\\\\n",
    "\\end{align}$$\n",
    "     \n",
    "     - 将结果代入目标式：$$\\begin{align}\n",
    "E[(x - \\mu_q)^T \\Sigma_q^{-1} (x - \\mu_q)] \n",
    "& = E[x^T \\Sigma_q^{-1} x] - 2\\mu_q^T \\Sigma_q^{-1} E[x] + \\mu_q^T \\Sigma_q^{-1} \\mu_q \\\\\n",
    "& = tr(\\Sigma_q^{-1}\\Sigma_p) + \\mu_p^T\\Sigma_q^{-1}\\mu_p\n",
    "- 2\\mu_q^T \\Sigma_q^{-1}\\mu_p + \\mu_q^T \\Sigma_q^{-1} \\mu_q \\\\\n",
    "& = tr(\\Sigma_q^{-1}\\Sigma_p) - (\\mu_p - \\mu_q)^T \\Sigma_q^{-1} (\\mu_p - \\mu_q)\n",
    "\\end{align}$$\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448037a5",
   "metadata": {},
   "source": [
    "3. **第二部分**: \n",
    "   - 直接代入第三部分计算结果：$$E[(x - \\mu_p)^T \\Sigma_p^{-1} (x - \\mu_p)] = \\text{tr}(\\Sigma_p^{-1} \\Sigma_p) + (\\mu_p - \\mu_p)^T \\Sigma_p^{-1} (\\mu_p - \\mu_p) = k$$     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98116962",
   "metadata": {},
   "source": [
    "#### 证明1：$Exx^T= \\Sigma + \\mu \\mu^T$\n",
    "$$\\begin{align}\n",
    "x\\sim N(\\mu, \\Sigma)时有：\n",
    "\\Sigma & = E(x-\\mu)(x-\\mu)^T\\\\\n",
    "& = E(xx^T-2ux^T+uu^T) \\\\\n",
    "& = Ex^Tx-u^Tu \\\\\n",
    "\\therefore Exx^T&= \\Sigma + \\mu \\mu^T \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aca36b5",
   "metadata": {},
   "source": [
    "#### 证明2：$\n",
    "E[x^T \\Sigma_q^{-1} x] = \\text{tr}(\\Sigma_q^{-1} E[xx^T])$\n",
    "$$\\begin{align}\n",
    "& \\because x^T \\Sigma_q^{-1} x是scalar \\\\\n",
    "& \\therefore x^T \\Sigma_q^{-1} x = tr(x^T\\Sigma_q^{-1} xx^T )=tr(\\Sigma_q^{-1} xx^T) \\\\\n",
    "& \\therefore E[x^T \\Sigma_q^{-1} x] = E[tr(\\Sigma_q^{-1} xx^T)]=tr(E[\\Sigma_q^{-1} xx^T])\n",
    "=tr(\\Sigma_q^{-1}E[xx^T])\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4259a31",
   "metadata": {},
   "source": [
    "### 附2：证明 \n",
    "$$logP(x;\\theta) = ELBO(x;Q(z|x),\\theta)+D_{KL}(Q(z|x)||P(z|x;\\theta))$$\n",
    "- 证明：\n",
    "$$\\begin{align}\n",
    "D_{KL}(Q(z|x_i)||P(z|x_i;{\\color{Red} \\theta}))\n",
    "& = \\int_zQ(z|x_i)log\\frac{Q(z|x_i)}{P(z|x_i;{\\color{Red} \\theta})}dz \\\\\n",
    "& = \\int_zQ(z|x_i)log\\frac{Q(z|x_i)P(x_i;{\\color{Red} \\theta})}{P(z,x_i;{\\color{Red} \\theta})}dz  \\\\\n",
    "& = \\int_zQ(z|x_i)\\left[ logP(x_i;{\\color{Red} \\theta}) -log\\frac{P(z,x_i;{\\color{Red} \\theta})}{Q(z|x_i)}\\right]dz  \\\\\n",
    "& = logP(x_i;{\\color{Red} \\theta})\\int_zQ(z|x_i)dz - \\int_zQ(z|x_i)log\\frac{P(z,x_i;{\\color{Red} \\theta})}{Q(z|x_i)}dz \\\\\n",
    "& = logP(x_i;{\\color{Red} \\theta}) - E_{z\\sim Q(z|x_i)}log\\frac{P(z,x_i;{\\color{Red} \\theta})}{Q(z|x_i)} \\\\\n",
    "& = logP(x_i;{\\color{Red} \\theta}) - E_{z\\sim Q(z|x_i)}log\\frac{P(x_i|z;{\\color{Red} \\theta} )P(z)}{Q(z|x_i)} \\\\\n",
    "& = logP(x_i;{\\color{Red} \\theta} ) - ELBO(x_i; Q,{\\color{Red} \\theta}  )\n",
    "\\end{align}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
