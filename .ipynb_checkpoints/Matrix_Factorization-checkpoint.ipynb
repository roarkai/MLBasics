{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c6ff70d-abf9-49e9-955f-b04ecae8d05f",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e47b1f6-f78d-4d65-81a0-46ece351f95d",
   "metadata": {},
   "source": [
    "五种典型方法：\n",
    "1. （略）$A=LU$， A是任意矩阵\n",
    "2. （略）QR分解：$A=QR$， A是任意矩阵\n",
    "3. 对称矩阵谱分解：$S=QΛQ^T$， S是对称矩阵\n",
    "4. （略）可逆矩阵分解：$A=XΛX^{-1}$， A是可逆矩阵\n",
    "5. 奇异值分解，SVD：$A=UΣV^T$， A是任意矩阵\n",
    "6. polar decomposition：A=SQ，A是任意矩阵，S是对称矩阵，Q是正交矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a4122-4e10-4df9-b29d-3123d6474727",
   "metadata": {},
   "source": [
    "## I. 对称矩阵的谱分解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b691ff13-d7a0-4bb6-83a1-84229c737f0d",
   "metadata": {},
   "source": [
    "### I.1 对称矩阵的一些性质\n",
    "#### $\\Diamond $ 对称矩阵的谱定理（4条）\n",
    "S是实对称矩阵，则：\n",
    "1. S有n个实特征值\n",
    "2. 每个特征值$\\lambda_i$的特征子空间的维数 = $\\lambda_i$作为特征方程$Ax=\\lambda x$的根的重数。\n",
    "3. 不同特征值对应的特征子空间相互正交\n",
    "4. S可以正交对角化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1212c3-85c9-4ce4-b5c0-774abb5187e9",
   "metadata": {},
   "source": [
    "#### $\\Diamond $ <font color=blue>可逆矩阵的对角化和对称矩阵的对角化的**充要条件**对比:</font>\n",
    "1. <font color=blue>矩阵是对称矩阵 $\\overset{i.f.f}{\\rightleftharpoons}$ 矩阵可以正交对角化：$S=Q\\Lambda Q^T$</font>\n",
    "2. <font color=blue>矩阵是可逆矩阵（即矩阵有n个线性无关特征向量）$\\overset{i.f.f}{\\rightleftharpoons}$ 矩阵可以对角化：$A=P\\Lambda P^{-1}$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2383c3-6e2b-4c38-9122-1fb191227172",
   "metadata": {},
   "source": [
    "### I.2 谱分解的形态\n",
    "对称矩阵S可以正交对角化为：$S=Q\\Lambda Q^T$。\\\n",
    "⑴ $\\Lambda$是对角矩阵，对角线上的元素是S的特征值。Q是特征值对应的特征向量矩阵。 \\\n",
    "⑵ Q是正交单位矩阵 \\\n",
    "⑶ S经过谱分解为最多d个rank=1的矩阵之和。如果存在$\\lambda_i=0$，那么对应的$\\lambda_iq_iq_i^T=0$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bbb110-90a5-4c62-865a-b19b470d1b10",
   "metadata": {},
   "source": [
    "$$\\begin{align} \n",
    "S_{d*d} & = Q^T\\Lambda Q \\\\\n",
    "&  =  \\begin{pmatrix}\n",
    " | & | &  & |\\\\\n",
    " q_1 & q_2 & ... & q_d\\\\\n",
    " | & | &  &|\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    " \\lambda_1 & 0 & ... & 0  \\\\\n",
    " 0 & \\lambda_2 &  & 0  \\\\\n",
    "  &  & ... &   \\\\\n",
    " 0 & 0 & ... & \\lambda_d  \n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "－& q_1^T & －\\\\\n",
    "－& q_2^T &－ \\\\\n",
    "－& ... & \\\\\n",
    "－& q_d^T & － \n",
    "\\end{pmatrix}  \\\\\n",
    "& = \\lambda _1q_1q_1^T + \\lambda _2q_2q_2^T + ... + \\lambda _dq_dq_d^T  \\\\\n",
    "& =  {\\textstyle \\sum_{i=1}^{d}} \\lambda _iq_iq_i^T  \\\\\n",
    "& 其中，Q^TQ =I_p, \\lambda_1>\\lambda_2>...>\\lambda_d\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7093bbc3-7c05-4f19-87a6-a24737ed9cec",
   "metadata": {},
   "source": [
    "## II. 任意矩阵的SVD分解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611a4047-224d-4b4f-a0b4-2aba61f6b352",
   "metadata": {},
   "source": [
    "### II.1 分解思路\n",
    "#### II.1.1 什么是SVD分解\n",
    "任意矩阵A都可以分解为：$A=U\\Sigma V^T$。\\\n",
    "① U是$AA^T$的特征向量矩阵，V是$A^TA$的特征向量矩阵。 \\\n",
    "② $\\Sigma$是$A^TA$和$AA^T$的特征值的平方根，即$\\Sigma=\\Lambda_{AA^T}^{\\frac{1}{2}}=\\Lambda_{A^TA}^{\\frac{1}{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f978d58c-d100-4404-9dee-da1f134467ae",
   "metadata": {},
   "source": [
    "#### II.1.2 SVD分解的两种形态\n",
    "1. SVD\n",
    "$$\\begin{align} \n",
    "A_{m*n} & = U\\Sigma V^T \\\\\n",
    "&  =  \\begin{pmatrix}\n",
    " | & | &  & |\\\\\n",
    " u_1 & u_2 & ... & u_m\\\\\n",
    " | & | &  &|\n",
    "\\end{pmatrix}_{m*m}\\begin{pmatrix}\n",
    " \\sigma_1 & 0 & ... & 0& & 0 \\\\\n",
    " 0 & \\sigma_2 &  & 0& ...&0 \\\\\n",
    "  &  & ... &  & \\\\\n",
    " 0 & 0 & ... & \\sigma_r  &...&0 \\\\\n",
    "  &  & ... &  & &\\\\\n",
    "0  &  & ... & 0 & ...&0\n",
    "\\end{pmatrix}_{m*n}\\begin{pmatrix}\n",
    "－& v_1^T & －\\\\\n",
    "－& v_2^T &－ \\\\\n",
    "－& ... & \\\\\n",
    "－& v_n^T & － \n",
    "\\end{pmatrix}_{n*n}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d7f9c5-b465-469d-843f-ce3ed022d3fa",
   "metadata": {},
   "source": [
    "2. 去掉多余信息的SVD\n",
    "$$\\begin{align} \n",
    "A_{m*n} & = U\\Sigma V^T \\\\\n",
    "&  =  \\begin{pmatrix}\n",
    " | & | &  & |\\\\\n",
    " u_1 & u_2 & ... & u_r\\\\\n",
    " | & | &  &|\n",
    "\\end{pmatrix}_{m*r}\\begin{pmatrix}\n",
    " \\sigma_1 & 0 & ... & 0 \\\\\n",
    " 0 & \\sigma_2 &  & 0 \\\\\n",
    "  &  & ... &  & \\\\\n",
    " 0 & 0 & ... & \\sigma_r\n",
    "\\end{pmatrix}_{r*r}\\begin{pmatrix}\n",
    "－& v_1^T & －\\\\\n",
    "－& v_2^T &－ \\\\\n",
    "－& ... & \\\\\n",
    "－& v_r^T & － \n",
    "\\end{pmatrix}_{r*n} \\\\\n",
    "& = \\sigma_1u_1v_1^T + \\sigma_2u_2v_2^T + ... + \\sigma_ru_rv_r^T  \\\\\n",
    "& =  {\\textstyle \\sum_{i=1}^{r}} \\sigma_iu_iv_i^T\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fae6e3-beaa-4853-babd-88765ded7045",
   "metadata": {},
   "source": [
    "#### II.1.3 SVD的分解的推导思路\n",
    "$\\diamond$ **$A^TA$和$AA^T$的性质**\n",
    "1. 性质1：$A^TA$和$AA^T$是半正定对称矩阵。\n",
    "2. 性质2：根据$A^TA$和$AA^T$有相同的特征值。\n",
    "3. 性质3：根据$A^TA$和$AA^T$的性质1和2，他们可以做谱分解，取：\\\n",
    "① $A^TA=U\\Lambda U^T$ \\\n",
    "② $AA^T=V\\Lambda V^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4478f5-15b0-49b8-83bd-9bd97501c34d",
   "metadata": {},
   "source": [
    "$\\diamond$ **SVD的分解的推导思路**\n",
    "(略)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b24318-541a-4f69-a68a-1e57d59730d2",
   "metadata": {},
   "source": [
    "### II.2 SVD的几何含义\n",
    "#### II.2.1 从SVD的视角看Ax的几何作用\n",
    "Ax对x做线性变换，从几何上看，$Ax=U\\Sigma V^Tx$可以分解成3步：\n",
    "1. $V^Tx$做rotation：V是单位正交矩阵，所以该运算不改变x的长度。如果同时对x和y做该运算，也不改变x和y的夹角大小。\n",
    "$$(V^Tx)^T(V^Ty)=x^TVV^Ty=x^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8d1484-a999-4d06-86aa-ff6e4132fff0",
   "metadata": {},
   "source": [
    "2. $\\Sigma (V^Tx)$做stretching：因为$\\Sigma$是对角矩阵，且对角线元素的值非负。此时，对角矩阵对向量的作用是在坐标轴的各个方向上做streching。\n",
    "$$\\Sigma z = \\begin{pmatrix}\n",
    " \\lambda_1 & 0 & ... & 0  \\\\\n",
    " 0 & \\lambda_2 &  & 0  \\\\\n",
    "  &  & ... &   \\\\\n",
    " 0 & 0 & ... & \\lambda_d  \n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "z_1 \\\\\n",
    "z_2 \\\\\n",
    "... \\\\\n",
    "z_d\n",
    "\\end{pmatrix}=\\begin{pmatrix}\n",
    " \\lambda_1z_1 \\\\\n",
    " \\lambda_2z_2 \\\\\n",
    "... \\\\\n",
    " \\lambda_dz_d\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6d3b7b-d1fd-4adc-8b55-0bdbe339b9f5",
   "metadata": {},
   "source": [
    "3. $U(\\Sigma V^Tx)$做rotation，同前面$V^Tx$的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e061f7-1e08-4873-9da6-0c6909047bd7",
   "metadata": {},
   "source": [
    "#### II.2.2 A的维度与rotation-strecting-rotation所需维度的对应关系\n",
    "1. 当A是2*2矩阵时，对应2维空间，A的所有信息包含在4个elements中。这4个elements共同提供了x的Transforming所需的4个信息，分别是：\\\n",
    "① streching需要2个信息，对应2维空间的x和y轴上各自strech的程度 \\\n",
    "② 在2维空间做rotation只需要指定1个角度信息，所以U的rotation需要1个信息，$V^T$的rotation也需要1个信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0868d6e-b867-4b92-9e28-d1c97860e19c",
   "metadata": {},
   "source": [
    "2. 当A是3*3矩阵时，对应3维空间，A的所有信息包含在9个elements中。这4个elements共同提供了x的Transforming所需的9个信息，分别是：\\\n",
    "① streching需要3个信息，对应3维空间的x，y，z轴上各自strech的程度 \\\n",
    "② 在3维空间做rotation只需要指定3个旋转方向信息，所以U的rotation需要3个信息，$V^T$的rotation也需要3个信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1793211-95e9-4eda-b550-bbe2195c93cc",
   "metadata": {},
   "source": [
    "3. 当A是4*4矩阵时，对应4维空间，A的所有信息包含在16个elements中。这16个elements共同提供了x的Transforming所需的16个信息，分别是：\\\n",
    "① streching需要4个信息，对应4维空间的4个轴上各自strech的程度 \\\n",
    "② 在4维空间做rotation只需要指定6个旋转方向信息，所以U的rotation需要3个信息，$V^T$的rotation也需要6个信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d5751-22ed-43d8-ac8d-826dea180335",
   "metadata": {},
   "source": [
    "### II.3 用SVD分析矩阵中的信息分布\n",
    "#### II.3.1 矩阵中的信息分布"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404393b7-a7ea-4d67-a737-f766af5f8ee2",
   "metadata": {},
   "source": [
    "1. <font color=blue>性质：如果A是秩为r的矩阵，则用SVD可以将A分解为r个秩为1的矩阵之和。</font>\n",
    "$$\\begin{align} \n",
    "A_{m*n} & = U\\Sigma V^T \\\\\n",
    "&  =  \\begin{pmatrix}\n",
    " | & | &  & |\\\\\n",
    " u_1 & u_2 & ... & u_r\\\\\n",
    " | & | &  &|\n",
    "\\end{pmatrix}_{m*r}\\begin{pmatrix}\n",
    " \\sigma_1 & 0 & ... & 0 \\\\\n",
    " 0 & \\sigma_2 &  & 0 \\\\\n",
    "  &  & ... &  & \\\\\n",
    " 0 & 0 & ... & \\sigma_r\n",
    "\\end{pmatrix}_{r*r}\\begin{pmatrix}\n",
    "－& v_1^T & －\\\\\n",
    "－& v_2^T &－ \\\\\n",
    "－& ... & \\\\\n",
    "－& v_r^T & － \n",
    "\\end{pmatrix}_{r*n} \\\\\n",
    "& = \\sigma_1u_1v_1^T + \\sigma_2u_2v_2^T + ... + \\sigma_ru_rv_r^T ,\\ 其中\\sigma_1 \\ge \\sigma_2 \\ge ... \\ge \\sigma_r \\\\\n",
    "& =  {\\textstyle \\sum_{i=1}^{r}} \\sigma_iu_iv_i^T \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a1e495-378a-4f55-9695-c39303e3504a",
   "metadata": {},
   "source": [
    "### II.3.2. 矩阵中的信息提取\n",
    "1. **性质**：<font color=blue>已知矩阵A的秩为r，根据SVD分解可知：$A= {\\textstyle \\sum_{i=1}^{r}} \\sigma_iu_iv_i^T $，其中：$\\sigma_1 \\ge \\sigma_2 \\ge ... \\ge \\sigma_r$。如果以矩阵的范数（norm of matrix）作为矩阵之间距离的亮度，则所有秩为k(k<r)的矩阵中$A= {\\textstyle \\sum_{i=1}^{k}} \\sigma_iu_iv_i^T $是离A最近的矩阵。</font> \\\n",
    "<font color=green>**Intuition**(这里直觉已经很好地说明了要义，证明略)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc763a0-736e-45d8-af2b-78d527e4c1f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a2357fa-d92e-44f7-a170-8edcdc3c7b1e",
   "metadata": {},
   "source": [
    "2. 补充：矩阵的范数（norm of matrix） \\\n",
    "⑴ 定义条件：所有类型的norm都应该满足两个条件：\\\n",
    "① $\\left \\| cx \\right \\| = c\\left \\| x \\right \\|$ \\\n",
    "② $\\left \\| x+y \\right \\| \\le \\left \\| x \\right \\| + \\left \\| y \\right \\|$ \\\n",
    "⑵ <font color=red>与distance区别：norm都是distance，但不是所有的distance都是norm</font> \\\n",
    "⑶ 几种常见的norm of matrix定义： \\\n",
    "① L2 norm: $\\left \\| A \\right \\|_2 = \\sigma _1$ \\\n",
    "② Frobenius norm: $\\left \\| A \\right \\|_F = \\sqrt{ {\\textstyle \\sum_{i,j}^{}}a^2_{ij}}=\\sqrt{{\\textstyle \\sum_{i=1}^{r}}\\sigma^2 _{i}}$ \\\n",
    "③ Nuclear norm: $\\left \\| A \\right \\|_N  = {\\textstyle \\sum_{i=1}^{r}}\\sigma _{i}$ \\\n",
    "<font color=blue>从这些norm of matrix的定义形式可见，他们都是由矩阵A的singular value决定的</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e44128-7ef6-41cd-b124-330bd3d53e7f",
   "metadata": {},
   "source": [
    "2. 理解：\\\n",
    "① A中最重要的信息存储在其最大的k个singular values中。\\\n",
    "② A中信息分散在r个秩为1的矩阵中，其中各个矩阵的信息含量排序为：$\\sigma_1u_1v^T_1 \\ge \\sigma_2u_2v^T_2 \\ge ...\\ge \\sigma_ru_rv^T_r$ \\\n",
    "③ A是秩为r的矩阵，如果要用秩为k($k\\le r$)的矩阵来尽可能多的包含A中信息。则可以取$A_k = {\\textstyle \\sum_{i=1}^{k}} \\sigma_iu_iv_i^T $，其中$\\sigma_1 \\ge \\sigma_2 \\ge ... \\ge \\sigma_r$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2477b948-48f3-45f9-9d67-0477bb12767e",
   "metadata": {},
   "source": [
    "## III. 任意矩阵的Polar decomposition\n",
    "### III.1 Polar decomposition：A=SQ\n",
    "思路：因为$A=U\\Sigma V^T$，所以$A=(U\\Sigma U^T)(UV^T)$，只要取$S=U\\Sigma U^T,Q=UV^T$就可以得到Polar decomposition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
