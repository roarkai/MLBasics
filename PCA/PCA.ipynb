{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44b7a92b",
   "metadata": {},
   "source": [
    "# PCA(principle component analysis)\n",
    "## I. PCA方法\n",
    "(ref:ESL, principle components)\n",
    "### 1.1 目标\n",
    "给高维空间上的向量找他们在低维空间上的**线性映射**$X_r^{'}=XU_r$，使得映射后得到的向量与原向量之间的$L_2 norm$最小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f8271f",
   "metadata": {},
   "source": [
    "### 1.2 投影知识基础\n",
    "1. **性质1：当w是单位向量时，$x^Tw=|x|·cos\\theta$是x在w方向上的投影长度。**\\\n",
    "<font color=orange>证明： \\\n",
    "根据几何性质：x在w方向上的投影大小(长度)是：$|x|·cos\\theta$，其中$\\theta$是向量x的方向与w方向的夹角。 \\\n",
    "根据内积特征：$x^Tw=|x|·|w|·cos\\theta$，当$|w|=1$时，$x^Tw=|x|·cos\\theta$ </font>\n",
    "2. **性质2：w是单位向量时，$(x^T)ww=(w^Tx)w=|x|·|w|·cos\\theta·w=|x|·cos\\theta·w$是x在w上的投影在原坐标系下的坐标。** \\\n",
    "<font color=orange>证明略</font>\n",
    "3. **性质3：坐标变换** \\\n",
    "<font color=red>注意符号：文中X的一行是一个样本，但对应的是$x_i^T，x_i\\in R^{d*1}$。</font> \\\n",
    "假如x是任意d维向量，在以d个<font color=blue>**单位正交**</font>向量$U_{d*d}=\\{u_1, u_2, ..., u_d\\}$为基的坐标系下，x可以表达为：\n",
    "$$x_i = UU^Tx_i$$<font color=red>注意这里的顺序是由x是列向量决定的</font> \\\n",
    "从矩阵角度来看就是：\n",
    "$$\n",
    "\\begin{align}\n",
    "X_{n*d} & = XUU^T \\\\\n",
    "x^T_i & = x_i^TUU^T ,x_i^T\\in R^{1*d}， 是X中的行向量 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "相当于将$X$投影到新的坐标系得到新坐标$X^{'}=XU$之后，再投回原坐标得到$X^{'}U^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c170a249",
   "metadata": {},
   "source": [
    "### 1.3 数学模型\n",
    "1. 假设 \\\n",
    "(1)原向量$X\\in R^d$，有n个样本$x_i,i\\in\\{1, 2, ..., n\\}$ \\\n",
    "(2)B是dxd维正交矩阵，其中r列$B_r$是$r*d$维矩阵，因此，$B_r^Tx_i$是$x_i$在$B_r$坐标空间上的新坐标\n",
    "2. 优化目标\n",
    "$$\n",
    "\\min_{B_r}\\ L = \\frac{1}{n}\\min_{B_r} \\sum_{i=1}^{n} ||x_i -B_rB_r^Tx_i||^2 \n",
    "$$\n",
    "3. 分析\n",
    "$$\n",
    "\\begin{align} \n",
    "L \n",
    "&= \\frac{1}{n}\\sum_{i=1}^{n} ||x_i -B_rB_r^Tx_i||^2 \\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^{n} ||BB^Tx_i -B_rB_r^Tx_i||^2 \\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^{n} \\left \\| \\sum_{k=1}^db_k(b_k^Tx_i)- \\sum_{k=1}^rb_k(b_k^Tx_i) \\right \\|^2  \\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^{n} \\left \\| \\sum_{k=r+1}^db_k(b_k^Tx_i) \\right \\|^2 \\\\  \n",
    "&= \\frac{1}{n}\\sum_{i=1}^{n} (\\sum_{k=r+1}^db_k(b_k^Tx_i))^T(\\sum_{k=r+1}^db_k(b_k^Tx_i)) \\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^{n} \\sum_{k=r+1}^d(b_k^Tx_i)b_k^Tb_k(b_k^Tx_i)，\\because u_k是单位正交基  \\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^{n}\\sum_{k=r+1}^d(x_i^Tb_k)^T(x_i^Tb_k)，\\because x_i^Tb_k是scalar \\\\\n",
    "&= \\sum_{k=r+1}^db_k^t(\\frac{1}{n}\\sum_{i=1}^{n}x_ix_i^T)b_k \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d30e70a",
   "metadata": {},
   "source": [
    "4. 数学处理：引入零均值化 \\\n",
    "<font color=red>本质上是为了数学计算的方便，此时求解该优化问题很容易。否则直接求解繁琐，且优化结果受X的均值影响。</font> \\\n",
    "(1)假设$x_i$是零均值化的 \\\n",
    "$$\n",
    "L = \\sum_{k=r+1}^db_k^t(\\frac{1}{n}\\sum_{i=1}^{n}x_ix_i^T)b_k = \\sum_{k=r+1}^db_k^tSb_k ,\\ S是X的协方差矩阵\n",
    "$$\n",
    "(2)从几何上理解零均值化处理的影响 \\\n",
    "a. $X^TB$的几何解释：PCA用正交矩阵做投影来实现降维，而正交矩阵投影运算本身在几何上起到的作用是对原数据集$X$做rotation \\\n",
    "b. 中心化可以简化rotation处理所需信息：从几何运动与矩阵运算的关系来看，以3D物体为例，中心化后的物体做rotation所需的信息最少，分别是水平方向rotation一次，垂直方向rotation一次，两个信息就能将该物体rotation到任意角度。但如果该物体没有中心化，需要的信息从直觉上看都会很复杂。\\\n",
    "c. 如果要在原位置上rotation最简便的方式是先中心化，然后rotation，然后将中点移动到原均值处。这正是PCA做零均值化处理使用的方式。本质上这样处理也可以在几何上让计算rotation的控制变量最简单化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03adf40",
   "metadata": {},
   "source": [
    "5. 求解零均值化后的优化问题：\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\underset{b_k}{argmin}\\ L = \\underset{b_k}{argmin}\\sum_{k=r+1}^db_k^tSb_k \\\\\n",
    "s.t.\\ b_k^Tb_k=1, b_k^Tb_j=0, (k,j=r+1, r+2, ..., d,且k\\ne j)\n",
    "\\end{cases}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "构建拉格朗日方程：& L=\\ {\\textstyle \\sum_{k={r+1}}^{d}} b_k^TSb_k +  {\\textstyle \\sum_{k={r+1}}^{d}} \\lambda_k(1- b_k^Tb_k) \\\\\n",
    "求解最优化：& \\frac{\\partial L}{\\partial b_k} = 2Sb_k-2\\lambda_k b_k = 0 \\\\\n",
    "即： & (S-\\lambda_k I)b_k  = 0 \\\\\n",
    "解必然满足：& \\lambda_k 是S的特征值，b_k是S的特征向量。\\\\\n",
    "分析：\\\\\n",
    "& \\because S = {\\textstyle \\sum_{i=1}^{d}} \\lambda _ig_ig_i^T,如果b_k=g_k \\\\\n",
    "& 则 \\sum_{k=r+1}^db_k^tSb_k= g_kSg_k^T = \\lambda _k \\\\\n",
    "& \\therefore b_{k,k=r+1,...,d}=\\underset{b_{k,k=r+1,...,d}}{argmin}\\ L = (g_{r+1}, g_{r+2}, ..., g_d)，是S最小的d-r个特征值的特征向量  \\\\\n",
    "& 此时，L=\\lambda_{r+1} + \\lambda_{r+2} + ...+ \\lambda_d\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7606d7e",
   "metadata": {},
   "source": [
    "6. 结果：\\\n",
    "a. $b_k$是S的特征向量，$b_k，k\\in \\{r+1, ..., d\\}$是S最小的$(d-r)$个特征值对应的特征向量。x在这些坐标上的投影被去掉（压缩）。\\\n",
    "b. 保留S最大的r个特征值的特征向量，构成矩阵$B_r$。$x_i$在他们上的投影$x_i^TB_r$被保留，作为降维后的向量$x_i^{'}\\in R^r$。\\\n",
    "c. 降维后的向量$x_i^{'}$在原坐标轴的坐标为$x_i^{'T}B_r^T$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d826a",
   "metadata": {},
   "source": [
    "## II. 方法步骤\n",
    "### 2.1 用谱分解\n",
    "1. 把所有原数据0均值化得到$\\tilde X_{n*d}=X_{n*d}-\\bar X_{n*d}=HX, 这里H是center\\ matrix，H=(I_n-\\frac{1}{n}1_n1_n^T)$ \\\n",
    "<font color=green>**center matrix H**有性质：\\\n",
    "a. $\\begin{align}\n",
    "& S = \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}(x_i-\\bar x)(x_i-\\bar x)^{T} = \\frac{1}{n}X^THX\n",
    "\\end{align}$ \\\n",
    "b. $H =H^T, H =H^2=H^n$ \\\n",
    "c. $HX=X-\\bar X=\\tilde X$，因此，H称为center matrix。这里$\\tilde x=x-\\bar x$是对向量做**中心化（零均值化）**。</font>\\\n",
    "<font color=red>注意区分中心化和归一化。后者有不同的计算方式，但通常指$x=\\frac{x-\\bar x}{\\max (x)-\\min (x)}$。</font>\n",
    "2. 计算协方差矩阵$S_{d*d}=\\frac{1}{n}X^THX$\n",
    "2. 用谱分解得到$S=Q^T\\Lambda Q$\n",
    "3. 选择S的特征值中最大的r个特征值$\\{\\lambda_1>\\lambda_2>...>\\lambda_r\\}$和对应的r个特征向量$Q_r=(q_1, q_2, ..., q_r), Q_r\\in R^{d*r}$\n",
    "4. 计算$\\tilde X$在$Q_r$上的投影$\\tilde X_r=\\tilde XQ_r, \\tilde X_r\\in R^{n*r}$\n",
    "5. 如果需要得到压缩后的向量在原空间中的坐标，用$\\tilde X_rQ_r^T\\in R^{n*d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1fc4fc",
   "metadata": {},
   "source": [
    "### 2.2 用SVD方法\n",
    "1. 把所有原数据0均值化得到$\\tilde X_{n*d}=X-\\tilde X=HX$\n",
    "2. 直接对$\\tilde X_{n*d}$做SVD分解得到$\\tilde X=U\\Sigma V^T$\n",
    "3. 取特征向量$Q=V\\in R^{d*d}$\n",
    "4. （后面步骤同前）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4d571e",
   "metadata": {},
   "source": [
    "### 2.3 奇异值分解与谱分解的关系\n",
    "1. SVD比谱分解的计算复杂度低 \\\n",
    "谱分解要先计算样本方差矩阵S，然后再计算谱分解。而奇异值分解可以不用计算样本方差，而直接通过SVD算法达到同样的效果。\n",
    "2. 用奇异值分解实现谱分解\\\n",
    "分析：\\\n",
    "$$\n",
    "\\begin{align} \n",
    "& \\tilde X = H_{n*n}X_{n*d}=U_{n*n}\\Sigma_{n*d}V_{d*d}^T \\begin{cases}\n",
    "U^TU=UU^T=I_n \\\\\n",
    "V^TV=VV^T=I_d \\\\\n",
    "\\Sigma 是对角矩阵(通常不是对称矩阵，即n\\ne d)\n",
    "\\end{cases}\\\\\n",
    "\\\\\n",
    "& S=\\frac{1}{n} X^THX =\\frac{1}{n} X^TH^THX=\\frac{1}{n} V\\Sigma^TU^TU\\Sigma V^T=V(\\frac{1}{n} \\Sigma^2 )V^T \\\\\n",
    "& 谱分解: S=G\\Lambda G^T，用V=G, \\Lambda=\\frac{1}{n} \\Sigma^2就可以得到谱分解结果\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f124b2e2",
   "metadata": {},
   "source": [
    "3. 更容易获得压缩结果: \\\n",
    "零均值化后的$\\tilde x_i$在新空间的坐标是：$\\tilde x_i^{T}V$。从整个样本上看，$\\tilde X$在新空间的坐标是: $\\tilde XV$\n",
    "$$\n",
    "\\begin{align} \n",
    "& \\because \\tilde X=X-\\bar X=HX \\\\\n",
    "& \\therefore \\tilde  XV=HXV=U\\Sigma V^TV=U\\Sigma \n",
    "\\end{align}$$\n",
    "同样可以直接得到$\\tilde X$在新空间压缩后的坐标：$\\tilde XV_r$\n",
    "$$\n",
    "\\tilde  XV_r=HXV_r=U\\Sigma V^TV_r=U\\Sigma I_r=U\\Sigma _r \\\\\n",
    "其中，U\\in R^{n*n}, \\Sigma_r \\in R^{n*r}\n",
    "$$\n",
    "这些降维后的向量在原空间中的坐标是:$\\tilde XV_rV_r^T$\n",
    "$$\n",
    "\\begin{align} \n",
    "\\tilde  XV_rV_r^T=U\\Sigma _rV_r^T \\\\\n",
    "\\Sigma_r \\in R^{n*r}, V_r^T \\in R^{r*d}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb54e13f",
   "metadata": {},
   "source": [
    "### 2.4 从SVD结果看norm度量信息损失的合理性\n",
    "1. <font color=blue>如果X是秩为r的矩阵，则用SVD可以将X分解为r个秩为1的矩阵之和。</font>\n",
    "$$\\begin{align} \n",
    "X_{m*n} & = U\\Sigma V^T \\\\\n",
    "&  =  \\begin{pmatrix}\n",
    " | & | &  & |\\\\\n",
    " u_1 & u_2 & ... & u_r\\\\\n",
    " | & | &  &|\n",
    "\\end{pmatrix}_{m*r}\\begin{pmatrix}\n",
    " \\sigma_1 & 0 & ... & 0 \\\\\n",
    " 0 & \\sigma_2 &  & 0 \\\\\n",
    "  &  & ... &  & \\\\\n",
    " 0 & 0 & ... & \\sigma_r\n",
    "\\end{pmatrix}_{r*r}\\begin{pmatrix}\n",
    "－& v_1^T & －\\\\\n",
    "－& v_2^T &－ \\\\\n",
    "－& ... & \\\\\n",
    "－& v_r^T & － \n",
    "\\end{pmatrix}_{r*n} \\\\\n",
    "& = \\sigma_1u_1v_1^T + \\sigma_2u_2v_2^T + ... + \\sigma_ru_rv_r^T ,\\ 其中\\sigma_1 \\ge \\sigma_2 \\ge ... \\ge \\sigma_r \\\\\n",
    "& =  {\\textstyle \\sum_{i=1}^{r}} \\sigma_iu_iv_i^T \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32787dc",
   "metadata": {},
   "source": [
    "2. 如果以矩阵的范数（norm of matrix）作为矩阵之间距离的量度，则所有秩为k(k<r)的矩阵中${\\textstyle \\sum_{i=1}^{k}} \\sigma_iu_iv_i^T $是离X最近的矩阵。 \\\n",
    "一个提供直觉的例子：\\\n",
    "取$\n",
    "\\Sigma=\\begin{pmatrix}\n",
    " 4 &  &  & 0\\\\\n",
    "  & 3 &  & \\\\\n",
    "  &  & 2 & \\\\\n",
    " 0 &  &  &1\n",
    "\\end{pmatrix}$，可以很容易得到结论：秩为2的离$\\Sigma$最近的矩阵是:$\\begin{align} \\Sigma_2=\\begin{pmatrix}\n",
    " 4 & 0 &  & \\\\\n",
    " 0 & 3 &  & \\\\\n",
    "  &  & 0 & 0\\\\\n",
    "  &  & 0 &0\n",
    "\\end{pmatrix}\n",
    "\\end{align}$ \\\n",
    "<font color=brown>假如$\\Sigma$是矩阵A的奇异值矩阵，即$A=U\\Sigma V^T$。那么如果要找距离A最近的秩为2的矩阵B，有：$B=U\\Sigma_2 V^T$。</font> \\\n",
    "原因：两个角度\\\n",
    "① 根据SVD的几何性质，U和V只是对中间的奇异值矩阵做rotation。rotation不改变norm所衡量的两个矩阵之间的距离。\\\n",
    "② 根据norm的定义，可以知道，如果U是单位正交矩阵，那么$\\left \\| UX \\right \\| = \\left \\| X \\right \\|$，因此有：$$\\left \\| UX-UY \\right \\| = \\left \\| U(X - Y) \\right \\| =\\left \\| X-Y \\right \\| $$\n",
    "可见，如果$\\Sigma_2$是距离$\\Sigma$最近的秩为2的矩阵，那么$U\\Sigma_2 V^T$也还是距离$U\\Sigma V^T$最近的秩为2的矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddaad76",
   "metadata": {},
   "source": [
    "## III. 最大投影方差\n",
    "### III.1 投影方差的定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13230e12",
   "metadata": {},
   "source": [
    "**定义**：n个<font color=green>零均值</font>的p维向量${x_1, x_2, ..., x_n}$在$w_k$方向上的投影方差记为$J_k$，有：\n",
    "$$J_k =\\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}(x_i^Tw_k)^2$$\n",
    "在K个方向上的投影方差之和为：$$\\begin{align} \n",
    "  J & =  {\\textstyle \\sum_{k=1}^{K}} J_k \\\\\n",
    "&= {\\textstyle \\sum_{k=1}^{K}}\\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}(x_i^Tw_k)^2 \\\\\n",
    "&=\\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}{\\textstyle \\sum_{k=1}^{K}}(x_i^Tw_k)^2\n",
    "\\end{align}$$\n",
    "<font color=red>**注：零均值条件**</font> \\\n",
    "① 零均值指向量已经做了中心化处理，此时${\\textstyle \\sum_{i=1}^{n}}x_i=0$ \\\n",
    "② 向量${x_i}$需要是零均值的，这样$\\frac{1}{n}{\\textstyle \\sum_{i=1}^{n}}(x^T_ix_i)$才是方差。相当于此时W=I，对应标准坐标轴。 \\\n",
    "③ 当向量${x_i}$需要是零均值时，向量${x_iW}$也是零均值的，因为：\n",
    "$${\\textstyle \\sum_{i=1}^{n}}(x^T_iW)=({\\textstyle \\sum_{i=1}^{n}}X^T_i)W=({\\textstyle \\sum_{i=1}^{n}}X_i)^TW=0\\cdot W=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a203f5b3",
   "metadata": {},
   "source": [
    "### III.2 理解投影方差\n",
    "#### $\\diamond $ 投影方差的性质\n",
    "1. <font color=blue>p维零均值向量的投影方差本质上是向量长度的平方的均值</font>$$J = \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}\\left \\| x_i \\right \\| _2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9f017-7ff5-42eb-8676-b36cf4a55405",
   "metadata": {},
   "source": [
    "2. <font color=blue>在任意p维单位正交坐标轴W下，它都是各向量在对应坐标下的L2 norm的均值</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a2bd7-0079-4d2b-b3b2-514202b31a51",
   "metadata": {},
   "source": [
    "$$J = \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}{\\textstyle \\sum_{k=1}^{p}}(x_i^Tw_k)^2\n",
    "= \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}\\left \\| x_i^TW \\right \\| _2^2 =\\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}\\left \\| \\tilde x_i \\right \\| _2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba67f35e-a7aa-407a-bba0-bd46202a0a29",
   "metadata": {},
   "source": [
    "3. 由2.可知，<font color=green>**投影方差的大小与坐标轴的选择无关。**</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bfcccb-0b37-4d2a-b1f7-4340207cee2d",
   "metadata": {},
   "source": [
    "4. p维零均值向量的投影方差 = W的p个坐标轴方向上的方差之和。$$J = {\\textstyle \\sum_{k=1}^{p}} J_k = {\\textstyle \\sum_{k=1}^{p}}\\left (\\frac{1}{n}{\\textstyle \\sum_{i=1}^{n}}(x_i^Tw_k)^2 \\right ) \\ , W=I时, J=  {\\textstyle \\sum_{k=1}^{p}}\\left(\\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}x_{i,k}^2\\right )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf487bbb-99c6-47f2-8afe-8c537c4abf7d",
   "metadata": {},
   "source": [
    "5. <font color=blue>p维零均值向量的投影方差 = 协方差矩阵的特征值之和</font>\n",
    "$$\\begin{align}\n",
    "J & =tr(S)={\\textstyle \\sum_{k=1}^{p}}\\lambda_k \\\\\n",
    "\\because S & = \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}(x_i-\\bar x)(x_i-\\bar x)^{T} \\\\\n",
    "& = \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}} x_i x_i^T \\ \\ (\\because  \\bar x=0)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb0a4be",
   "metadata": {},
   "source": [
    "#### 1.3.2 方差度量信息大小\n",
    "1. 根据定义，$tr(S)=\\sum_{k=1}^{d} Var(x_k)$。S是方阵，因此$tr(S)=\\sum_{j=1}^{d}\\lambda_j$。所以，$\\sum_{k=1}^{d} Var(x_k)=\\sum_{j=1}^{d}\\lambda_j$\n",
    "2. X在S的特征向量$g_i,i\\in\\{1, 2, ..., d\\}$上的投影的方差是$\\lambda_i$。（详见后文）\n",
    "3. 如果用方差衡量矩阵信息量的大小，那么所有d维正交向量中，选择S的特征值中最大的r个对应的特征向量为坐标，得到X的投影的方差之和最大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0713099f-0611-475d-b6b5-400b91c29ab0",
   "metadata": {},
   "source": [
    "#### $\\diamond$ 投影方差与坐标轴的关系\n",
    "1. 坐标系是标准坐标系时 \\\n",
    "   n个<font color=red>**中心化（零均值化）后**</font>的p维向量${x_1, x_2, ..., x_n}$在标准坐标$e_k$方向上的投影方差为：\n",
    "   $$J_k =\\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}(x_i^Te_k)^2= \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}x_{i,k}^2$$在所有p个维度上的投影方差之和：$$J =  {\\textstyle \\sum_{k=1}^{p}} J_k =\\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}{\\textstyle \\sum_{k=1}^{p}}x_{i,k}^2 =\\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}\\left \\| x_i \\right \\| _2^2\n",
    "   $$\n",
    "<font color=blue>**由于此时向量已经中心化，所以向量长度平方的均值也是样本的方差。**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658ab933-ae0c-49bb-a6e0-98acf781f248",
   "metadata": {},
   "source": [
    "2. 一般化到任意<font color=blue>单位正交</font>坐标系W \\\n",
    "   向量$x_i$在新坐标系下的坐标为$\\tilde x_i=x_i^TW$，n个向量X在新坐标系下的坐标为$\\tilde X=XW$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90410c94-2da1-4919-8b3d-23d87e11d9d8",
   "metadata": {},
   "source": [
    "$$\\begin{align} \n",
    "  J & = \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}{\\textstyle \\sum_{k=1}^{p}}(x_i^Tw_k)^2 \\\\\n",
    "&= \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}\\left \\| x_i^TW \\right \\| _2^2 (\\because  W是单位正交矩阵)\\\\\n",
    "&=\\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}\\left \\| x_i \\right \\| _2^2 \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b26ce-8a5a-4a39-9529-32024f06969a",
   "metadata": {},
   "source": [
    "3. 如果只是投影到W中的K维空间，坐标方向是$W_K，且K<p$，则投影方差之和是各个向量在新的k维坐标系下的新坐标的L2 norm的均值。\n",
    "$$\\begin{align} \n",
    "  J_K & = \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}{\\textstyle \\sum_{k=1}^{K}}(x_i^Tw_k)^2 \\\\\n",
    "&= \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}\\left \\| x_i^TW_K \\right \\| _2^2 \\\\\n",
    "& \\le J\n",
    "\\end{align}\n",
    "$$\n",
    "<font color=green>**PCA的思路就是早到合适的$W_k$，让$J_k$与$J$的差距尽可能小。**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f1e016",
   "metadata": {},
   "source": [
    "### III. 计算最大投影方差\n",
    "1. **计算最大投影方差**：先中心化后再计算，$x_i'=x_i-\\bar x$ \\\n",
    "将X投影到向量$w_k$上，投影方差为：\n",
    "$$\\begin{align} \n",
    "J_k & =\\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}(x_i^{'T}w_k)^2 \\\\\n",
    "& = \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}((x-\\bar x)^{T}w_k)^2 \\\\\n",
    "& = \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}((x-\\bar x)^{T}w_k)^T((x-\\bar x)^{T}w_k) \\\\\n",
    "& = \\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}(w_k^{T}(x-\\bar x))((x-\\bar x)^{T}w_k) \\\\\n",
    "& = w_k^{T}(\\frac{1}{n}\\ {\\textstyle \\sum_{i=1}^{n}}(x-\\bar x)(x-\\bar x)^{T})w_k \\\\\n",
    "& = w_k^{T}Sw_k \\\\\n",
    "\\\\\n",
    "\\end{align} $$\n",
    "如果将X投影到任意K个w向量为坐标构成的空间上，则投影方差合计：\n",
    "$$\\begin{align} \n",
    "J & =  {\\textstyle \\sum_{k=1}^{K}} J(w_k) = {\\textstyle \\sum_{k=1}^{K}}w_k^{T}Sw_k  \\\\\n",
    "\\end{align}$$\n",
    "<font color=red>说明：这里直接加总$x_i$在各个K个$w_k$上的投影，暗含的假设是：$w_i \\perp w_j，if\\ i\\ne j$。也就是说，目标函数中已经假设了找到的各个最小投影方差的方向是相互正交的。原因详见后文：可以直接假设新坐标中的基是单位正交基</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c540ca41",
   "metadata": {},
   "source": [
    "2. **求解产生最大投影方差的方向** \\\n",
    "思路: 转化为优化问题 \\\n",
    "$$\\begin{align} \n",
    "&   \\hat w = \\underset{W}{argmax}\\ J(W) =\\underset{w_1, ..., w_K}{argmax}\\  {\\textstyle \\sum_{k=1}^{K}} w_k^TSw_k \\\\\n",
    "&  s.t\\ \\ w_k^Tw_k= 1, w_k^Tw_j=0, \\ （k,j = 1, 2, ..., K,且k\\ne j）\n",
    "\\end{align}$$\n",
    "$$\\begin{align} \n",
    "构建拉格朗日方程：& L(W, \\lambda ) = \\  {\\textstyle \\sum_{k=1}^{K}} w_k^TSw_k +  {\\textstyle \\sum_{k=1}^{K}} \\lambda_k(1- w_k^Tw_k) \\\\\n",
    " \\\\\n",
    "求解最优化：& \\frac{\\partial L}{\\partial w_k} = 2Sw_k-2\\lambda_k w_k = 0 \\\\\n",
    "即： & (S-\\lambda_k I)w_k  = 0 \\\\\n",
    "解必然满足：& \\lambda_k 是S的特征值，w_k是S的特征向量。\n",
    "\\end{align}$$\n",
    "$$\n",
    "\\begin{align} \n",
    "& \\because S = {\\textstyle \\sum_{i=1}^{p}} \\lambda _ig_ig_i^T \\\\\n",
    "& \\therefore J(g_j) = g_j^TSg_j = \\lambda _j \\\\\n",
    "& \\therefore \\hat W = argmax\\ J(W) = (g_1, g_2, ..., g_K)，是S最大的K个特征值的特征向量  \\\\\n",
    "& 此时，J(w)=\\lambda_1 + \\lambda_2 + ...+ \\lambda_K\n",
    "\\end{align}$$\n",
    "**结论：** \\\n",
    "向量$x_i^{'}$在新空间上的坐标是$x_i^{'T}W$，也就是$x_i^{'}$在各个基向量$g_k$上的投影长度。得到的投影向量有最大投影方差。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
